[
	{
		"session_id": "9202596c-d296-440a-b61f-1fca4bc5b4ea",
		"transcript_path": "/Users/promptune/.claude/projects/-Users-promptune-DevProjects-promptune/9202596c-d296-440a-b61f-1fca4bc5b4ea.jsonl",
		"cwd": "/Users/promptune/DevProjects/promptune",
		"permission_mode": "default",
		"hook_event_name": "PostToolUse",
		"tool_name": "Read",
		"tool_input": {
			"file_path": "/Users/promptune/DevProjects/promptune/lib/semantic_router_matcher.py"
		},
		"tool_response": {
			"type": "text",
			"file": {
				"filePath": "/Users/promptune/DevProjects/promptune/lib/semantic_router_matcher.py",
				"content": "#!/usr/bin/env -S uv run --script\n# /// script\n# requires-python = \">=3.10\"\n# dependencies = [\n#     \"semantic-router>=0.1.0\"\n# ]\n# ///\n\n\"\"\"\nSemantic Router-based intent matcher for SuperClaude command detection.\n\nProvides fast, semantic matching of user queries to SuperClaude commands using\nembeddings-based routing with Cohere or OpenAI encoders.\n\nTarget performance: 50-100ms per query\n\"\"\"\n\nimport os\nimport time\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\nfrom semantic_router import Route, SemanticRouter\nfrom semantic_router.encoders import CohereEncoder, OpenAIEncoder\n\n\n@dataclass\nclass IntentMatch:\n    \"\"\"Result of semantic intent matching.\"\"\"\n    command: str\n    confidence: float\n    method: str  # \"semantic_router\"\n    latency_ms: float\n    matched_patterns: List[str]\n\n\nclass SemanticRouterMatcher:\n    \"\"\"\n    Semantic Router-based intent matcher for SuperClaude commands.\n\n    Uses embeddings-based semantic routing to match user queries to commands\n    with high accuracy and low latency. Supports both Cohere and OpenAI encoders.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize matcher with lazy loading.\"\"\"\n        self._router: Optional[SemanticRouter] = None\n        self._encoder_type: Optional[str] = None\n        self._routes: List[Route] = self._define_routes()\n\n    def _define_routes(self) -> List[Route]:\n        \"\"\"\n        Define semantic routes for SuperClaude commands.\n\n        Each route contains example utterances that represent the command's intent.\n        \"\"\"\n        return [\n            Route(\n                name=\"/sc:analyze\",\n                utterances=[\n                    \"analyze this code\",\n                    \"review the code quality\",\n                    \"check for code issues\",\n                    \"what are the problems with this code\",\n                    \"audit this codebase\",\n                    \"assess code quality\",\n                    \"inspect the architecture\",\n                    \"evaluate the code structure\",\n                    \"find security vulnerabilities\",\n                    \"check for performance issues\",\n                    \"review this implementation\",\n                    \"what's wrong with this code\"\n                ]\n            ),\n            Route(\n                name=\"/sc:test\",\n                utterances=[\n                    \"run the tests\",\n                    \"execute test suite\",\n                    \"check test coverage\",\n                    \"run unit tests\",\n                    \"test this code\",\n                    \"verify with tests\",\n                    \"check if tests pass\",\n                    \"measure code coverage\",\n                    \"run integration tests\",\n                    \"execute all tests\",\n                    \"validate with test cases\",\n                    \"generate test report\"\n                ]\n            ),\n            Route(\n                name=\"/sc:troubleshoot\",\n                utterances=[\n                    \"fix this bug\",\n                    \"debug this issue\",\n                    \"why isn't this working\",\n                    \"solve this problem\",\n                    \"troubleshoot the error\",\n                    \"diagnose this issue\",\n                    \"find the bug\",\n                    \"repair this code\",\n                    \"resolve this error\",\n                    \"figure out what's broken\",\n                    \"investigate this failure\",\n                    \"why is this failing\"\n                ]\n            ),\n            Route(\n                name=\"/sc:implement\",\n                utterances=[\n                    \"build a new feature\",\n                    \"implement this functionality\",\n                    \"create a new component\",\n                    \"develop this feature\",\n                    \"add this capability\",\n                    \"build this from scratch\",\n                    \"implement the requirements\",\n                    \"develop this module\",\n                    \"create this feature\",\n                    \"build the functionality\",\n                    \"add this feature\",\n                    \"write the implementation\"\n                ]\n            ),\n            Route(\n                name=\"/sc:explain\",\n                utterances=[\n                    \"explain this code\",\n                    \"what does this do\",\n                    \"help me understand this\",\n                    \"document this code\",\n                    \"describe how this works\",\n                    \"clarify this implementation\",\n                    \"break down this code\",\n                    \"walk me through this\",\n                    \"explain the logic\",\n                    \"what's happening here\",\n                    \"help me understand the flow\",\n                    \"describe the architecture\"\n                ]\n            ),\n            Route(\n                name=\"/sc:improve\",\n                utterances=[\n                    \"optimize this code\",\n                    \"refactor this implementation\",\n                    \"improve performance\",\n                    \"make this code better\",\n                    \"enhance this code\",\n                    \"clean up this code\",\n                    \"improve code quality\",\n                    \"optimize for speed\",\n                    \"refactor for maintainability\",\n                    \"enhance the performance\",\n                    \"make this more efficient\",\n                    \"improve the design\"\n                ]\n            )\n        ]\n\n    def _get_encoder(self):\n        \"\"\"\n        Get appropriate encoder based on available API keys.\n\n        Tries Cohere first, then OpenAI. Returns None if no keys available.\n        \"\"\"\n        cohere_key = os.environ.get(\"COHERE_API_KEY\")\n        openai_key = os.environ.get(\"OPENAI_API_KEY\")\n\n        if cohere_key:\n            self._encoder_type = \"cohere\"\n            return CohereEncoder(cohere_api_key=cohere_key)\n        elif openai_key:\n            self._encoder_type = \"openai\"\n            return OpenAIEncoder(openai_api_key=openai_key)\n        else:\n            return None\n\n    def _build_router(self) -> bool:\n        \"\"\"\n        Build the router with lazy initialization.\n\n        Returns:\n            True if router was built successfully, False otherwise.\n        \"\"\"\n        if self._router is not None:\n            return True\n\n        try:\n            encoder = self._get_encoder()\n            if encoder is None:\n                return False\n\n            self._router = SemanticRouter(\n                encoder=encoder,\n                routes=self._routes\n            )\n            return True\n        except Exception:\n            # Gracefully handle encoder initialization errors\n            return False\n\n    def is_available(self) -> bool:\n        \"\"\"\n        Check if semantic router is available.\n\n        Returns:\n            True if API key is available and router can be built.\n        \"\"\"\n        if self._router is not None:\n            return True\n\n        cohere_key = os.environ.get(\"COHERE_API_KEY\")\n        openai_key = os.environ.get(\"OPENAI_API_KEY\")\n\n        return cohere_key is not None or openai_key is not None\n\n    def match(self, text: str) -> Optional[IntentMatch]:\n        \"\"\"\n        Match user query to SuperClaude command using semantic routing.\n\n        Args:\n            text: User query text to match.\n\n        Returns:\n            IntentMatch object if successful match found, None otherwise.\n        \"\"\"\n        # Lazy build router on first use\n        if not self._build_router():\n            return None\n\n        start_time = time.perf_counter()\n\n        try:\n            # Route the query\n            route_choice = self._router(text)\n\n            # Calculate latency\n            latency_ms = (time.perf_counter() - start_time) * 1000\n\n            # semantic-router returns RouteChoice with name field\n            if route_choice is None or route_choice.name is None:\n                return None\n\n            route_name = route_choice.name\n\n            # Get matched route for patterns\n            matched_route = next(\n                (route for route in self._routes if route.name == route_name),\n                None\n            )\n\n            # Extract sample patterns from the matched route\n            matched_patterns = (\n                matched_route.utterances[:3] if matched_route else []\n            )\n\n            # Use similarity_score if available, otherwise default confidence\n            confidence = route_choice.similarity_score if route_choice.similarity_score else 0.85\n\n            return IntentMatch(\n                command=route_name,\n                confidence=confidence,\n                method=\"semantic_router\",\n                latency_ms=latency_ms,\n                matched_patterns=matched_patterns\n            )\n\n        except Exception:\n            # Gracefully handle API errors or routing failures\n            return None\n\n\n# Example usage and testing\nif __name__ == \"__main__\":\n    matcher = SemanticRouterMatcher()\n\n    if not matcher.is_available():\n        print(\"No API key available. Set COHERE_API_KEY or OPENAI_API_KEY.\")\n        exit(1)\n\n    # Test queries\n    test_queries = [\n        \"Can you review this code for me?\",\n        \"Run all the tests please\",\n        \"This function is broken, help me fix it\",\n        \"Build a login system\",\n        \"What does this function do?\",\n        \"Make this code faster\"\n    ]\n\n    print(\"Testing Semantic Router Matcher\\n\" + \"=\" * 50)\n\n    for query in test_queries:\n        result = matcher.match(query)\n\n        if result:\n            print(f\"\\nQuery: {query}\")\n            print(f\"Command: {result.command}\")\n            print(f\"Confidence: {result.confidence:.2f}\")\n            print(f\"Latency: {result.latency_ms:.2f}ms\")\n            print(f\"Matched patterns: {result.matched_patterns[:2]}\")\n        else:\n            print(f\"\\nQuery: {query}\")\n            print(\"No match found\")\n",
				"numLines": 307,
				"startLine": 1,
				"totalLines": 307
			}
		},
		"tool_use_id": "toolu_016mf36vEtvb1njhKZTEY7EC"
	},
	{
		"session_id": "9202596c-d296-440a-b61f-1fca4bc5b4ea",
		"transcript_path": "/Users/promptune/.claude/projects/-Users-promptune-DevProjects-promptune/9202596c-d296-440a-b61f-1fca4bc5b4ea.jsonl",
		"cwd": "/Users/promptune/DevProjects/promptune",
		"permission_mode": "default",
		"hook_event_name": "PostToolUse",
		"tool_name": "Grep",
		"tool_input": {
			"pattern": "LOG_LEVEL|log_level|SEMANTIC_ROUTER",
			"output_mode": "content",
			"-i": true
		},
		"tool_response": {
			"mode": "content",
			"numFiles": 0,
			"filenames": [],
			"content": ".mcp.json:29:    //     \"LOG_LEVEL\": \"DEBUG\",\nCLAUDE.md:102:\u2502   \u251c\u2500\u2500 semantic_router_matcher.py # Tier 3: LLM-based (50ms)\nCLAUDE.md:113:\u2502   \u251c\u2500\u2500 test_semantic_router.py\nCLAUDE.md:174:uv run lib/semantic_router_matcher.py\nlogs/cb49dd14-0259-4321-8860-bd671b0081f6/chat.json:6:[Omitted long matching line]\nlogs/cb49dd14-0259-4321-8860-bd671b0081f6/chat.json:26:[Omitted long matching line]\nlogs/cb49dd14-0259-4321-8860-bd671b0081f6/chat.json:157:[Omitted long matching line]\nPLUGIN_ARCHITECTURE.md:30:\u2502   \u251c\u2500\u2500 semantic_router_matcher.py\nREADME.md:575:uv run lib/semantic_router_matcher.py\nREADME.md:630:    \"semantic_router\": false\nREADME.md:753:uv run lib/semantic_router_matcher.py\nREADME.md:808:Or disable in config: `\"semantic_router\": false`\ndocs/delegation-modes.md:141:    \"semantic_router\": true\nlogs/3d6fcd4a-b74a-4d5f-94a2-0c8bcdc5516f/post_tool_use.json:129:[Omitted long matching line]\nlogs/3d6fcd4a-b74a-4d5f-94a2-0c8bcdc5516f/post_tool_use.json:373:[Omitted long matching line]\nlogs/9202596c-d296-440a-b61f-1fca4bc5b4ea/pre_tool_use.json:10:      \"file_path\": \"/Users/promptune/DevProjects/promptune/lib/semantic_router_matcher.py\"\nlogs/9202596c-d296-440a-b61f-1fca4bc5b4ea/pre_tool_use.json:22:      \"pattern\": \"LOG_LEVEL|log_level|SEMANTIC_ROUTER\",\nlogs/3d6fcd4a-b74a-4d5f-94a2-0c8bcdc5516f/chat.json:1140:[Omitted long matching line]\nlogs/3d6fcd4a-b74a-4d5f-94a2-0c8bcdc5516f/chat.json:1150:[Omitted long matching line]\nlogs/3d6fcd4a-b74a-4d5f-94a2-0c8bcdc5516f/chat.json:2515:[Omitted long matching line]",
			"numLines": 20,
			"appliedLimit": 20
		},
		"tool_use_id": "toolu_01HKKh6x6Azx7ekcdFZLANLK"
	},
	{
		"session_id": "9202596c-d296-440a-b61f-1fca4bc5b4ea",
		"transcript_path": "/Users/promptune/.claude/projects/-Users-promptune-DevProjects-promptune/9202596c-d296-440a-b61f-1fca4bc5b4ea.jsonl",
		"cwd": "/Users/promptune/DevProjects/promptune",
		"permission_mode": "default",
		"hook_event_name": "PostToolUse",
		"tool_name": "Bash",
		"tool_input": {
			"command": "env | grep -i \"log\\|semantic\"",
			"description": "Check for log-related environment variables"
		},
		"tool_response": {
			"stdout": "OSLogRateLimit=64\nLOGNAME=promptune\nLOGFIRE_TOKEN=pylf_v1_us_bySxdNZ9h4VSgk2cc5Zrfj2bhdbltZGYFL4jLmCVRc3d\nLOGFIRE_READ_TOKEN=pylf_v1_us_md81D5bD8x6HcPzPk2Rn2N7H4GJ7NQPxlpnxR1Cy7Rbd",
			"stderr": "",
			"interrupted": false,
			"isImage": false
		},
		"tool_use_id": "toolu_015LmuJQRYAnDU1Gj1LFL1ug"
	},
	{
		"session_id": "9202596c-d296-440a-b61f-1fca4bc5b4ea",
		"transcript_path": "/Users/promptune/.claude/projects/-Users-promptune-DevProjects-promptune/9202596c-d296-440a-b61f-1fca4bc5b4ea.jsonl",
		"cwd": "/Users/promptune/DevProjects/promptune",
		"permission_mode": "default",
		"hook_event_name": "PostToolUse",
		"tool_name": "Glob",
		"tool_input": {
			"pattern": "**/.env*"
		},
		"tool_response": {
			"filenames": [
				"/Users/promptune/DevProjects/promptune/.env.example",
				"/Users/promptune/DevProjects/promptune/.env"
			],
			"durationMs": 527,
			"numFiles": 2,
			"truncated": false
		},
		"tool_use_id": "toolu_018z1GCXh2mnxm1j9agErhmx"
	},
	{
		"session_id": "9202596c-d296-440a-b61f-1fca4bc5b4ea",
		"transcript_path": "/Users/promptune/.claude/projects/-Users-promptune-DevProjects-promptune/9202596c-d296-440a-b61f-1fca4bc5b4ea.jsonl",
		"cwd": "/Users/promptune/DevProjects/promptune",
		"permission_mode": "default",
		"hook_event_name": "PostToolUse",
		"tool_name": "Glob",
		"tool_input": {
			"pattern": "**/marimo*.py"
		},
		"tool_response": {
			"filenames": [
				"/Users/promptune/DevProjects/promptune/.venv/lib/python3.13/site-packages/marimo/_runtime/marimo_browser.py",
				"/Users/promptune/DevProjects/promptune/.venv/lib/python3.13/site-packages/marimo/_runtime/marimo_pdb.py",
				"/Users/promptune/DevProjects/promptune/.venv/lib/python3.13/site-packages/marimo/_utils/marimo_path.py",
				"/Users/promptune/DevProjects/promptune/marimo_tutorial_sql.py"
			],
			"durationMs": 516,
			"numFiles": 4,
			"truncated": false
		},
		"tool_use_id": "toolu_01ErgfDEeEbheXXYzNogseqJ"
	},
	{
		"session_id": "9202596c-d296-440a-b61f-1fca4bc5b4ea",
		"transcript_path": "/Users/promptune/.claude/projects/-Users-promptune-DevProjects-promptune/9202596c-d296-440a-b61f-1fca4bc5b4ea.jsonl",
		"cwd": "/Users/promptune/DevProjects/promptune",
		"permission_mode": "default",
		"hook_event_name": "PostToolUse",
		"tool_name": "Read",
		"tool_input": {
			"file_path": "/Users/promptune/DevProjects/promptune/marimo_tutorial_sql.py"
		},
		"tool_response": {
			"type": "text",
			"file": {
				"filePath": "/Users/promptune/DevProjects/promptune/marimo_tutorial_sql.py",
				"content": "# Copyright 2025 Marimo. All rights reserved.\n# /// script\n# requires-python = \">=3.12\"\n# dependencies = [\n#     \"duckdb==1.2.2\",\n#     \"marimo>=0.17.0\",\n#     \"pandas==2.2.3\",\n#     \"polars==1.27.1\",\n#     \"pyarrow==19.0.1\",\n#     \"pyzmq\",\n#     \"sqlglot==26.13.0\",\n# ]\n# ///\n\nimport marimo\n\n__generated_with = \"0.18.0\"\napp = marimo.App(width=\"medium\")\n\n\n@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"\n    # Hello, SQL!\n\n    _Let's dive into the world of SQL where we don't just address tables, we also join them!_\n    \"\"\")\n    return\n\n\n@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"\n    With marimo, you can mix-and-match both **Python and SQL**. To create a\n    SQL cell, you first need to install some additional dependencies,\n    including [duckdb](https://duckdb.org/). Obtain these dependencies with\n\n    ```bash\n    pip install 'marimo[sql]'\n    ```\n    \"\"\")\n    return\n\n\n@app.cell(hide_code=True)\ndef _():\n    has_duckdb_installed = False\n    try:\n        import duckdb\n\n        has_duckdb_installed = True\n    except ImportError:\n        pass\n\n    has_polars_installed = False\n    try:\n        import polars\n\n        has_polars_installed = True\n    except ImportError:\n        pass\n\n    has_pandas_installed = False\n    try:\n        import pandas\n\n        has_pandas_installed = True\n    except ImportError:\n        pass\n    return has_duckdb_installed, has_polars_installed\n\n\n@app.cell(hide_code=True)\ndef _(has_duckdb_installed, mo):\n    if has_duckdb_installed:\n        mo.output.replace(\n            mo.md(\n                \"\"\"\n                /// Tip | \"Installed\"\n\n                    If you see this, DuckDB is already installed.\n                ///\n                \"\"\"\n            )\n        )\n    else:\n        mo.output.replace(\n            mo.md(\n                \"\"\"\n                /// Warning | \"Not Installed\"\n                    If you see this, DuckDB is not installed.\n                ///\n                \"\"\"\n            )\n        )\n    return\n\n\n@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"\n    ## Creating SQL cells\n\n    Once the required dependencies are installed, you can create SQL cells\n    in one of the following ways:\n\n    - right click the **Add Cell** ::lucide:circle-plus:: buttons on the left of\n    a cell;\n    -  click the **Convert to SQL** ::lucide:database:: button in the cell menu ::lucide:ellipsis::\n    -  click the **Add SQL Cell** at the bottom of the page;\n\n    ## Python representation\n    marimo is still just Python, even when using SQL. Here is an example of\n    how marimo embeds SQL in Python in its file format:\n\n    ```python\n    output_df = mo.sql(f\"SELECT * FROM my_table LIMIT {max_rows.value}\")\n    ```\n\n    Notice that we have an **`output_df`** variable in the cell. This is a\n    resulting Polars DataFrame (if you have `polars` installed) or a Pandas\n    DataFrame (if you don't). One of them must be installed in order to\n    interact with the SQL result.\n\n    The SQL statement itself is an formatted string (f-string), so this\n    means they can contain any valid Python code, such as the values of UI\n    elements. This means your SQL statement and results can be reactive! \ud83d\ude80\n    \"\"\")\n    return\n\n\n@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(\"\"\"\n    ## Querying dataframes with SQL\n    \"\"\")\n    return\n\n\n@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"\n    /// Tip | \"Data sources panel\"\n\n        Click the database \"barrel\" icon in the left toolbar to see all dataframes and in-\n        memory tables that your notebook has access to.\n    ///\n    \"\"\")\n    return\n\n\n@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"\n    Let's take a look at a SQL cell. The next cell generates a dataframe called `df`.\n    \"\"\")\n    return\n\n\n@app.cell(hide_code=True)\ndef _(has_polars_installed):\n    _SIZE = 1000\n\n\n    def _create_token_data(n_items=100):\n        import random\n        import string\n\n        def generate_random_string(length):\n            letters = string.ascii_lowercase\n            result_str = \"\".join(random.choice(letters) for i in range(length))\n            return result_str\n\n        def generate_random_numbers(mean, std_dev, num_samples):\n            return [int(random.gauss(mean, std_dev)) for _ in range(num_samples)]\n\n        random_numbers = generate_random_numbers(50, 15, n_items)\n        random_strings = sorted(\n            list(set([generate_random_string(3) for _ in range(n_items)]))\n        )\n\n        return {\n            \"token\": random_strings,\n            \"count\": random_numbers[: len(random_strings)],\n        }\n\n\n    _data = _create_token_data(_SIZE)\n\n    # Try polars\n    if has_polars_installed:\n        import polars as pl\n\n        df = pl.DataFrame(_data)\n    # Fallback to pandas (maybe trying to install it)\n    else:\n        import pandas as pd\n\n        df = pd.DataFrame(_data)\n    return\n\n\n@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"\n    Next, we create a SQL query, referencing the Python dataframe `df` directly.\n    \"\"\")\n    return\n\n\n@app.cell\ndef _(mo):\n    _df = mo.sql(\n        f\"\"\"\n        -- This SQL cell is special since we can reference existing dataframes in the global scope as a table in the SQL query. For example, we can reference the `df` dataframe in the global scope, which was defined in another cell using Python.\n\n        SELECT * FROM df;\n\n        -- By default, the output variable starts with an underscore (`_df`), making it private to this cell. To access the query result in another cell, change the name of the output variable.\n        \"\"\"\n    )\n    return\n\n\n@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(\"\"\"\n    ## From Python to SQL and back\n    \"\"\")\n    return\n\n\n@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"\n    You can create SQL statements that depend on Python values, such as UI elements:\n    \"\"\")\n    return\n\n\n@app.cell(hide_code=True)\ndef _(mo, string):\n    token_prefix = mo.ui.dropdown(\n        list(string.ascii_lowercase), label=\"token prefix\", value=\"a\"\n    )\n    token_prefix\n    return (token_prefix,)\n\n\n@app.cell\ndef _(mo, token_prefix):\n    result = mo.sql(\n        f\"\"\"\n        -- Change the dropdown to see the SQL query filter itself!\n        --\n        -- Here we use a duckdb function called `starts_with`:\n        SELECT * FROM df WHERE starts_with(token, '{token_prefix.value}')\n\n        -- Notice that we named the output variable `result`\n        \"\"\"\n    )\n    return (result,)\n\n\n@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"\n    Since we named the output variable above **`result`**,\n    we can use it back in Python.\n    \"\"\")\n    return\n\n\n@app.cell(hide_code=True)\ndef _(mo):\n    charting_library = mo.ui.radio([\"matplotlib\", \"altair\", \"plotly\"])\n\n    mo.md(\n        f\"\"\"\n        Let's chart the result with a library of your choice:\n\n        {charting_library}\n        \"\"\"\n    )\n    return (charting_library,)\n\n\n@app.cell(hide_code=True)\ndef _(charting_library, mo, render_chart, token_prefix):\n    _header = mo.md(\n        f\"\"\"\n        We can re-use the dropdown from above: {token_prefix}\n\n        Now we have a histogram visualizing the token count distribution of tokens starting\n        with {token_prefix.value}, powered by your SQL query and UI element.\n        \"\"\"\n    )\n\n    render_chart(\n        charting_library.value, _header\n    ) if charting_library.value else None\n    return\n\n\n@app.cell(hide_code=True)\ndef _(mo, result, token_prefix):\n    def render_chart(charting_library, header):\n        return mo.vstack(\n            [header, render_charting_library(charting_library)]\n        ).center()\n\n\n    def render_charting_library(charting_library):\n        if charting_library == \"matplotlib\":\n            return render_matplotlib()\n        if charting_library == \"altair\":\n            return render_altair()\n        if charting_library == \"plotly\":\n            return render_plotly()\n\n\n    def render_matplotlib():\n        import matplotlib.pyplot as plt\n\n        plt.hist(result[\"count\"], label=token_prefix.value)\n        plt.xlabel(\"token count\")\n        plt.legend()\n        plt.tight_layout()\n        return plt.gcf()\n\n\n    def render_altair():\n        import altair as alt\n\n        chart = (\n            alt.Chart(result)\n            .mark_bar()\n            .encode(x=alt.X(\"count\", bin=True), y=alt.Y(\"count()\"))\n        )\n        return mo.ui.altair_chart(chart, chart_selection=False)\n\n\n    def render_plotly():\n        import plotly.graph_objects as go\n\n        return go.Figure(data=[go.Histogram(x=result[\"count\"])])\n    return (render_chart,)\n\n\n@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"\n    ## CSVs, Parquet, Postgres, and more ...\n    \"\"\")\n    return\n\n\n@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"\n    We're not limited to querying dataframes. We can also query an **HTTP URL, S3 path, or a file path to a local csv or parquet file**.\n\n    ```sql\n    -- or\n    SELECT * FROM 's3://my-bucket/file.parquet';\n    -- or\n    SELECT * FROM read_csv('path/to/example.csv');\n    -- or\n    SELECT * FROM read_parquet('path/to/example.parquet');\n    ```\n\n    With a bit of boilerplate, you can even read and write to **Postgres**, and join Postgres tables with dataframes in the same query. For a full list of supported data sources, check out the [duckdb extensions](https://duckdb.org/docs/extensions/overview) and our [example notebook on duckdb connections](https://github.com/marimo-team/marimo/blob/main/examples/sql/duckdb_connections.**py**).\n\n    For this example, we will query an HTTP endpoint of a csv.\n    \"\"\")\n    return\n\n\n@app.cell\ndef _(mo):\n    cars = mo.sql(\n        f\"\"\"\n        -- Download a CSV and create an in-memory table; this is optional.\n        CREATE OR replace TABLE cars as\n        FROM 'https://datasets.marimo.app/cars.csv';\n\n        -- Query the table\n        SELECT * from cars;\n        \"\"\"\n    )\n    return (cars,)\n\n\n@app.cell(hide_code=True)\ndef _(cars, mo):\n    cylinders_dropdown = mo.ui.range_slider.from_series(\n        cars[\"Cylinders\"], debounce=True, show_value=True\n    )\n    origin_dropdown = mo.ui.dropdown.from_series(cars[\"Origin\"], value=\"Asia\")\n    mo.hstack([cylinders_dropdown, origin_dropdown]).left()\n    return cylinders_dropdown, origin_dropdown\n\n\n@app.cell\ndef _(cylinders_dropdown, mo, origin_dropdown):\n    filtered_cars = mo.sql(\n        f\"\"\"\n        SELECT * FROM cars\n        WHERE\n            Cylinders >= {cylinders_dropdown.value[0]}\n            AND\n            Cylinders <= {cylinders_dropdown.value[1]}\n            AND\n            ORIGIN = '{origin_dropdown.value}'\n        \"\"\"\n    )\n    return (filtered_cars,)\n\n\n@app.cell(hide_code=True)\ndef _(filtered_cars, mo):\n    mo.hstack(\n        [\n            mo.stat(label=\"Total cars\", value=str(len(filtered_cars))),\n            mo.stat(\n                label=\"Average MPG Highway\",\n                value=f\"{filtered_cars['MPG_Highway'].mean() or 0:.1f}\",\n            ),\n            mo.stat(\n                label=\"Average MPG City\",\n                value=f\"{filtered_cars['MPG_City'].mean() or 0:.1f}\",\n            ),\n        ]\n    )\n    return\n\n\n@app.cell(hide_code=True)\ndef _():\n    import marimo as mo\n    import random\n    return (mo,)\n\n\n@app.cell(hide_code=True)\ndef _():\n    import string\n    return (string,)\n\n\nif __name__ == \"__main__\":\n    app.run()\n",
				"numLines": 453,
				"startLine": 1,
				"totalLines": 453
			}
		},
		"tool_use_id": "toolu_01ULrS1ahEjC4bemxUzr156q"
	},
	{
		"session_id": "9202596c-d296-440a-b61f-1fca4bc5b4ea",
		"transcript_path": "/Users/promptune/.claude/projects/-Users-promptune-DevProjects-promptune/9202596c-d296-440a-b61f-1fca4bc5b4ea.jsonl",
		"cwd": "/Users/promptune/DevProjects/promptune",
		"permission_mode": "default",
		"hook_event_name": "PostToolUse",
		"tool_name": "Bash",
		"tool_input": {
			"command": "echo 'export SEMANTIC_ROUTER_LOG_LEVEL=\"INFO\"' >> ~/.zshrc && export SEMANTIC_ROUTER_LOG_LEVEL=\"INFO\"",
			"description": "Set semantic router log level"
		},
		"tool_response": {
			"stdout": "",
			"stderr": "",
			"interrupted": false,
			"isImage": false
		},
		"tool_use_id": "toolu_0199w3A7cycLvZSa695WyGTx"
	},
	{
		"session_id": "9202596c-d296-440a-b61f-1fca4bc5b4ea",
		"transcript_path": "/Users/promptune/.claude/projects/-Users-promptune-DevProjects-promptune/9202596c-d296-440a-b61f-1fca4bc5b4ea.jsonl",
		"cwd": "/Users/promptune/DevProjects/promptune",
		"permission_mode": "default",
		"hook_event_name": "PostToolUse",
		"tool_name": "TodoWrite",
		"tool_input": {
			"todos": [
				{
					"content": "Fix semantic_router hook crash with graceful error handling",
					"status": "in_progress",
					"activeForm": "Adding graceful error handling to hook"
				},
				{
					"content": "Show user how to use mo.hstack() for side-by-side layout",
					"status": "pending",
					"activeForm": "Demonstrating mo.hstack() usage"
				}
			]
		},
		"tool_response": {
			"oldTodos": [],
			"newTodos": [
				{
					"content": "Fix semantic_router hook crash with graceful error handling",
					"status": "in_progress",
					"activeForm": "Adding graceful error handling to hook"
				},
				{
					"content": "Show user how to use mo.hstack() for side-by-side layout",
					"status": "pending",
					"activeForm": "Demonstrating mo.hstack() usage"
				}
			]
		},
		"tool_use_id": "toolu_01FruKw68aGzpPPKML3EG7Tr"
	},
	{
		"session_id": "9202596c-d296-440a-b61f-1fca4bc5b4ea",
		"transcript_path": "/Users/promptune/.claude/projects/-Users-promptune-DevProjects-promptune/9202596c-d296-440a-b61f-1fca4bc5b4ea.jsonl",
		"cwd": "/Users/promptune/DevProjects/promptune",
		"permission_mode": "default",
		"hook_event_name": "PostToolUse",
		"tool_name": "Read",
		"tool_input": {
			"file_path": "/Users/promptune/DevProjects/promptune/hooks/user_prompt_submit.py",
			"limit": 50
		},
		"tool_response": {
			"type": "text",
			"file": {
				"filePath": "/Users/promptune/DevProjects/promptune/hooks/user_prompt_submit.py",
				"content": "#!/usr/bin/env -S uv run --script\n# /// script\n# requires-python = \">=3.10\"\n# dependencies = [\n#     \"model2vec>=0.3.0\",\n#     \"semantic-router>=0.1.0\",\n#     \"numpy>=1.24.0\",\n#     \"rapidfuzz>=3.0.0\"\n# ]\n# ///\n\"\"\"\nPromptune UserPromptSubmit Hook\n\nDetects slash commands from natural language prompts using 3-tier cascade:\n1. Keyword matching (0.02ms, 60% coverage)\n2. Model2Vec embeddings (0.2ms, 30% coverage)\n3. Semantic Router (50ms, 10% coverage)\n\nUses Claude Code headless mode for interactive prompt analysis and suggestions.\n\nHook Protocol:\n- Input: JSON via stdin with {\"prompt\": \"...\", \"session_id\": \"...\"}\n- Output: JSON via stdout with {\"continue\": true, \"feedback\": \"...\"}\n\"\"\"\n\nimport json\nimport subprocess\nimport sys\nimport re\nfrom pathlib import Path\nfrom typing import Any\n\n# Add lib directory to Python path\nPLUGIN_ROOT = Path(__file__).parent.parent\nsys.path.insert(0, str(PLUGIN_ROOT / \"lib\"))\n\n# Import matchers (now using RapidFuzz-based keyword matcher v2!)\nfrom keyword_matcher_v2 import IntentMatch, KeywordMatcherV2 as KeywordMatcher\nfrom model2vec_matcher import Model2VecMatcher\nfrom observability_db import ObservabilityDB\nfrom semantic_router_matcher import SemanticRouterMatcher\n\n\nclass PromptuneDetector:\n    \"\"\"\n    3-tier intent detection cascade.\n\n    Uses your existing matchers in order of speed:\n    1. KeywordMatcher (always fast)\n    2. Model2VecMatcher (if available)",
				"numLines": 50,
				"startLine": 1,
				"totalLines": 915
			}
		},
		"tool_use_id": "toolu_018aTnigeVDKwJ7ee5bvz9fC"
	},
	{
		"session_id": "9202596c-d296-440a-b61f-1fca4bc5b4ea",
		"transcript_path": "/Users/promptune/.claude/projects/-Users-promptune-DevProjects-promptune/9202596c-d296-440a-b61f-1fca4bc5b4ea.jsonl",
		"cwd": "/Users/promptune/DevProjects/promptune",
		"permission_mode": "acceptEdits",
		"hook_event_name": "PostToolUse",
		"tool_name": "Edit",
		"tool_input": {
			"file_path": "/Users/promptune/DevProjects/promptune/hooks/user_prompt_submit.py",
			"old_string": "# Import matchers (now using RapidFuzz-based keyword matcher v2!)\nfrom keyword_matcher_v2 import IntentMatch, KeywordMatcherV2 as KeywordMatcher\nfrom model2vec_matcher import Model2VecMatcher\nfrom observability_db import ObservabilityDB\nfrom semantic_router_matcher import SemanticRouterMatcher",
			"new_string": "# Import matchers (now using RapidFuzz-based keyword matcher v2!)\nfrom keyword_matcher_v2 import IntentMatch, KeywordMatcherV2 as KeywordMatcher\nfrom model2vec_matcher import Model2VecMatcher\nfrom observability_db import ObservabilityDB\n\n# Gracefully handle semantic_router import errors (logging issues, etc.)\ntry:\n    from semantic_router_matcher import SemanticRouterMatcher\n    SEMANTIC_ROUTER_AVAILABLE = True\nexcept Exception as e:\n    print(f\"Warning: semantic_router unavailable: {e}\", file=sys.stderr)\n    SemanticRouterMatcher = None\n    SEMANTIC_ROUTER_AVAILABLE = False"
		},
		"tool_response": {
			"filePath": "/Users/promptune/DevProjects/promptune/hooks/user_prompt_submit.py",
			"oldString": "# Import matchers (now using RapidFuzz-based keyword matcher v2!)\nfrom keyword_matcher_v2 import IntentMatch, KeywordMatcherV2 as KeywordMatcher\nfrom model2vec_matcher import Model2VecMatcher\nfrom observability_db import ObservabilityDB\nfrom semantic_router_matcher import SemanticRouterMatcher",
			"newString": "# Import matchers (now using RapidFuzz-based keyword matcher v2!)\nfrom keyword_matcher_v2 import IntentMatch, KeywordMatcherV2 as KeywordMatcher\nfrom model2vec_matcher import Model2VecMatcher\nfrom observability_db import ObservabilityDB\n\n# Gracefully handle semantic_router import errors (logging issues, etc.)\ntry:\n    from semantic_router_matcher import SemanticRouterMatcher\n    SEMANTIC_ROUTER_AVAILABLE = True\nexcept Exception as e:\n    print(f\"Warning: semantic_router unavailable: {e}\", file=sys.stderr)\n    SemanticRouterMatcher = None\n    SEMANTIC_ROUTER_AVAILABLE = False",
			"originalFile": "#!/usr/bin/env -S uv run --script\n# /// script\n# requires-python = \">=3.10\"\n# dependencies = [\n#     \"model2vec>=0.3.0\",\n#     \"semantic-router>=0.1.0\",\n#     \"numpy>=1.24.0\",\n#     \"rapidfuzz>=3.0.0\"\n# ]\n# ///\n\"\"\"\nPromptune UserPromptSubmit Hook\n\nDetects slash commands from natural language prompts using 3-tier cascade:\n1. Keyword matching (0.02ms, 60% coverage)\n2. Model2Vec embeddings (0.2ms, 30% coverage)\n3. Semantic Router (50ms, 10% coverage)\n\nUses Claude Code headless mode for interactive prompt analysis and suggestions.\n\nHook Protocol:\n- Input: JSON via stdin with {\"prompt\": \"...\", \"session_id\": \"...\"}\n- Output: JSON via stdout with {\"continue\": true, \"feedback\": \"...\"}\n\"\"\"\n\nimport json\nimport subprocess\nimport sys\nimport re\nfrom pathlib import Path\nfrom typing import Any\n\n# Add lib directory to Python path\nPLUGIN_ROOT = Path(__file__).parent.parent\nsys.path.insert(0, str(PLUGIN_ROOT / \"lib\"))\n\n# Import matchers (now using RapidFuzz-based keyword matcher v2!)\nfrom keyword_matcher_v2 import IntentMatch, KeywordMatcherV2 as KeywordMatcher\nfrom model2vec_matcher import Model2VecMatcher\nfrom observability_db import ObservabilityDB\nfrom semantic_router_matcher import SemanticRouterMatcher\n\n\nclass PromptuneDetector:\n    \"\"\"\n    3-tier intent detection cascade.\n\n    Uses your existing matchers in order of speed:\n    1. KeywordMatcher (always fast)\n    2. Model2VecMatcher (if available)\n    3. SemanticRouterMatcher (if API key available)\n    \"\"\"\n\n    def __init__(self):\n        self._keyword = None\n        self._model2vec = None\n        self._semantic = None\n\n    def _get_keyword(self):\n        if self._keyword is None:\n            self._keyword = KeywordMatcher()\n        return self._keyword\n\n    def _get_model2vec(self):\n        if self._model2vec is None:\n            m = Model2VecMatcher()\n            self._model2vec = m if m.is_available() else None\n        return self._model2vec\n\n    def _get_semantic(self):\n        if self._semantic is None:\n            m = SemanticRouterMatcher()\n            self._semantic = m if m.is_available() else None\n        return self._semantic\n\n    def detect(self, text: str) -> IntentMatch | None:\n        \"\"\"Detect intent using 3-tier cascade.\"\"\"\n\n        # Tier 1: Keyword (always available)\n        result = self._get_keyword().match(text)\n        if result:\n            return result\n\n        # Tier 2: Model2Vec\n        m2v = self._get_model2vec()\n        if m2v:\n            result = m2v.match(text)\n            if result:\n                return result\n\n        # Tier 3: Semantic Router\n        sem = self._get_semantic()\n        if sem:\n            result = sem.match(text)\n            if result:\n                return result\n\n        return None\n\n\nclass ClaudeCodeHaikuEngineer:\n    \"\"\"\n    Uses Claude Code headless mode to analyze prompts and provide interactive suggestions.\n\n    Benefits:\n    - No separate API key needed (uses existing Claude Code auth)\n    - Integrated billing with Claude Code\n    - Fast Haiku model for cost optimization\n    - Interactive blocking mode for user feedback\n    \"\"\"\n\n    def __init__(self):\n        self._claude_available = None\n\n    def is_available(self) -> bool:\n        \"\"\"Check if Claude Code CLI is available.\"\"\"\n        if self._claude_available is None:\n            try:\n                result = subprocess.run(\n                    [\"claude\", \"--version\"], capture_output=True, text=True, timeout=2\n                )\n                self._claude_available = result.returncode == 0\n            except (FileNotFoundError, subprocess.TimeoutExpired):\n                self._claude_available = False\n\n        return self._claude_available\n\n    def analyze_and_enhance(\n        self,\n        prompt: str,\n        detected_command: str,\n        confidence: float,\n        available_commands: list[str],\n        timeout: int = 30,\n    ) -> dict[str, Any] | None:\n        \"\"\"\n        Analyze prompt using Claude Code headless mode and suggest enhancements.\n\n        Args:\n            prompt: User's original prompt\n            detected_command: Command detected by cascade\n            confidence: Detection confidence (0-1)\n            available_commands: List of all available commands\n            timeout: Timeout in seconds\n\n        Returns:\n            Dict with analysis results or None if unavailable/failed\n        \"\"\"\n        if not self.is_available():\n            return None\n\n        # Build analysis prompt for Haiku\n        analysis_prompt = f\"\"\"You are a prompt enhancement assistant for Promptune, a Claude Code plugin.\n\nUSER'S PROMPT: \"{prompt}\"\n\nDETECTED COMMAND: {detected_command}\nDETECTION CONFIDENCE: {confidence:.0%}\n\nAVAILABLE ALTERNATIVES:\n{chr(10).join(f\"- {cmd}\" for cmd in available_commands[:10])}\n\nTASK: Analyze the user's prompt and provide:\n1. Whether the detected command is the best match (true/false)\n2. Alternative commands if better matches exist\n3. A brief, helpful suggestion for the user\n\nRESPONSE FORMAT (JSON):\n{{\n  \"is_best_match\": true/false,\n  \"alternatives\": [\"command1\", \"command2\"],\n  \"suggestion\": \"Brief suggestion text\"\n}}\n\nBe concise. Focus on actionability.\"\"\"\n\n        try:\n            # Call Claude Code headless with Haiku model\n            cmd = [\n                \"claude\",\n                \"--model\",\n                \"claude-haiku-4-5\",\n                \"-p\",\n                analysis_prompt,\n                \"--output-format\",\n                \"json\",\n                \"--allowedTools\",\n                \"\",  # No tools needed for this analysis\n            ]\n\n            result = subprocess.run(\n                cmd, capture_output=True, text=True, timeout=timeout\n            )\n\n            if result.returncode != 0:\n                print(f\"DEBUG: Claude Code error: {result.stderr}\", file=sys.stderr)\n                return None\n\n            # Parse Claude's response - it's nested in a wrapper object\n            claude_response = json.loads(result.stdout)\n\n            # Extract the actual result (may be nested in \"result\" field)\n            if \"result\" in claude_response:\n                result_text = claude_response[\"result\"]\n                # Result may contain JSON in markdown code blocks\n                if \"```json\" in result_text:\n                    # Extract JSON from markdown code block\n                    json_start = result_text.find(\"```json\") + 7\n                    json_end = result_text.find(\"```\", json_start)\n                    result_text = result_text[json_start:json_end].strip()\n\n                # Parse the extracted JSON\n                analysis = json.loads(result_text)\n                return analysis\n            else:\n                # If no \"result\" field, assume the whole response is the analysis\n                return claude_response\n\n        except subprocess.TimeoutExpired:\n            print(f\"DEBUG: Claude Code timeout after {timeout}s\", file=sys.stderr)\n            return None\n        except json.JSONDecodeError as e:\n            print(f\"DEBUG: Failed to parse Claude response: {e}\", file=sys.stderr)\n            print(f\"DEBUG: Raw output: {result.stdout[:200]}\", file=sys.stderr)\n            return None\n        except Exception as e:\n            print(f\"DEBUG: Haiku engineer error: {e}\", file=sys.stderr)\n            return None\n\n\ndef should_process(prompt: str) -> bool:\n    \"\"\"Check if prompt needs intent detection.\"\"\"\n    if not prompt or not prompt.strip():\n        return False\n\n    # Skip if already a command\n    if prompt.strip().startswith(\"/\"):\n        return False\n\n    # Skip internal Haiku analysis prompts (prevent feedback loop)\n    if prompt.startswith(\"You are a prompt enhancement assistant\"):\n        return False\n\n    # Skip if too short\n    if len(prompt.strip().split()) < 3:\n        return False\n\n    return True\n\n\ndef write_detection_for_statusline(match: IntentMatch, prompt: str):\n    \"\"\"Write detection data to observability DB for status line to read.\"\"\"\n    try:\n        db = ObservabilityDB(\".promptune/observability.db\")\n        db.set_detection(\n            command=match.command,\n            confidence=match.confidence,\n            method=match.method,\n            prompt_preview=prompt[:60] + (\"...\" if len(prompt) > 60 else \"\"),\n            latency_ms=match.latency_ms,\n        )\n\n        # Also log matcher performance\n        db.log_matcher_performance(match.method, match.latency_ms, success=True)\n\n        print(\n            f\"DEBUG: Wrote detection to observability DB: {match.command} ({match.confidence:.0%} {match.method})\",\n            file=sys.stderr,\n        )\n    except Exception as e:\n        # Don't fail hook if observability write fails\n        print(f\"DEBUG: Failed to write to observability DB: {e}\", file=sys.stderr)\n        # Also log the error\n        try:\n            db = ObservabilityDB(\".promptune/observability.db\")\n            db.log_error(\"user_prompt_submit\", type(e).__name__, str(e))\n        except:\n            pass\n\n\ndef clear_detection_statusline():\n    \"\"\"Clear status line detection (no match found).\"\"\"\n    try:\n        db = ObservabilityDB(\".promptune/observability.db\")\n        db.clear_detection()\n        print(\"DEBUG: Cleared detection from observability DB\", file=sys.stderr)\n    except Exception as e:\n        print(\n            f\"DEBUG: Failed to clear detection from observability DB: {e}\",\n            file=sys.stderr,\n        )\n\n\ndef get_detection_count() -> int:\n    \"\"\"Get total number of detections for progressive tips.\"\"\"\n    try:\n        db = ObservabilityDB(\".promptune/observability.db\")\n        stats = db.get_stats()\n        return stats.get(\"detections\", {}).get(\"total\", 0)\n    except:\n        pass\n    return 0\n\n\ndef increment_detection_count():\n    \"\"\"Increment detection counter for progressive disclosure.\"\"\"\n    try:\n        data_dir = Path.home() / \".claude\" / \"plugins\" / \"promptune\" / \"data\"\n        data_dir.mkdir(parents=True, exist_ok=True)\n        stats_file = data_dir / \"detection_stats.json\"\n\n        stats = {\"total_detections\": 0, \"by_method\": {}, \"by_command\": {}}\n        if stats_file.exists():\n            with open(stats_file) as f:\n                stats = json.load(f)\n\n        stats[\"total_detections\"] = stats.get(\"total_detections\", 0) + 1\n\n        with open(stats_file, \"w\") as f:\n            json.dump(stats, f, indent=2)\n    except:\n        pass  # Don't fail hook if stats tracking fails\n\n\n# Command action descriptions for directive feedback\nCOMMAND_ACTIONS = {\n    # Promptune commands\n    \"/ctx:design\": \"design system architecture with structured workflow\",\n    \"/ctx:research\": \"get fast answers using 3 parallel agents\",\n    \"/ctx:plan\": \"create parallel development plans\",\n    \"/ctx:execute\": \"run tasks in parallel worktrees\",\n    \"/ctx:status\": \"monitor parallel task progress\",\n    \"/ctx:cleanup\": \"clean up completed worktrees\",\n    \"/ctx:help\": \"see example-first command guide\",\n    \"/ctx:configure\": \"enable persistent status bar display\",\n    \"/ctx:stats\": \"see your time & cost savings\",\n    \"/ctx:verify\": \"verify and execute detected command with confirmation\",\n    # Skill-only detections (no commands)\n    \"skill:ctx:performance\": \"analyze and optimize parallel workflow performance\",\n    \"skill:ctx:parallel-expert\": \"get guidance on parallelizing tasks effectively\",\n    \"skill:ctx:help\": \"discover Promptune features and capabilities\",\n    \"skill:ctx:worktree\": \"troubleshoot git worktree issues and conflicts\",\n}\n\n# Skill mapping for reliable Claude execution\n# Maps slash commands AND skill detections to skill names\n# Skills are auto-discovered by Claude Code from: promptune/skills/*/SKILL.md\nSKILL_MAPPING = {\n    # Commands with skills\n    \"/ctx:design\": \"ctx:architect\",  # Plugin skill: skills/software-architect\n    \"/ctx:research\": \"ctx:researcher\",  # Plugin skill: skills/researcher\n    # Skills without commands (direct skill suggestions)\n    \"skill:ctx:performance\": \"ctx:performance\",\n    \"skill:ctx:parallel-expert\": \"ctx:parallel-expert\",\n    \"skill:ctx:help\": \"ctx:help\",\n    \"skill:ctx:worktree\": \"ctx:worktree\",\n    # Note: /ctx:plan and /ctx:execute are commands, not skills\n    # They execute workflows directly rather than providing guidance\n}\n\n\ndef create_skill_augmented_prompt(match: IntentMatch, original_prompt: str) -> str:\n    \"\"\"\n    Augment prompt with skill suggestion for more reliable execution.\n\n    Evidence: Skills are invoked more reliably than slash commands because\n    they use Claude's native Skill tool (structured, type-safe) vs text expansion.\n\n    Args:\n        match: Detected command and confidence\n        original_prompt: User's original prompt text\n\n    Returns:\n        Augmented prompt that guides Claude to use skill or command\n    \"\"\"\n    if match.command in SKILL_MAPPING:\n        skill_name = SKILL_MAPPING[match.command]\n        # Strong directive: \"You can use your X skill\"\n        return f\"{original_prompt}. You can use your {skill_name} skill to help with this task.\"\n    else:\n        # For commands without skills, use directive language\n        action = COMMAND_ACTIONS.get(match.command, \"complete this request\")\n        return f\"{original_prompt}. Please use the {match.command} command to {action}.\"\n\n\ndef get_contextual_tip(match: IntentMatch, detection_count: int) -> str:\n    \"\"\"Generate directive contextual tip based on usage patterns.\"\"\"\n\n    # First-time users (1-3 detections)\n    if detection_count <= 3:\n        return \"New user? Type `/ctx:help` to see all commands with examples\"\n\n    # Early users (4-10 detections) - promote status bar\n    elif detection_count <= 10:\n        return \"Enable persistent detection: Type `/ctx:configure` to set up status bar\"\n\n    # Experienced users (11-20) - promote advanced features\n    elif detection_count <= 20:\n        if match.command.startswith(\"/ctx:\"):\n            return \"Want parallel workflows? Type `/ctx:plan` to work on multiple tasks simultaneously\"\n        return f\"Blazing fast: {match.latency_ms:.2f}ms detection. Type `/ctx:stats` to see all metrics\"\n\n    # Power users (21+) - occasional celebration\n    else:\n        if detection_count % 10 == 0:  # Every 10th detection\n            return f\"\ud83c\udf89 {detection_count} detections! Type `/ctx:stats` to see your time & cost savings\"\n        return None  # No tip for most interactions\n\n\ndef load_available_commands() -> list[str]:\n    \"\"\"Load list of all available commands for Claude Code.\"\"\"\n    # Return all commands from COMMAND_ACTIONS\n    return [cmd for cmd in COMMAND_ACTIONS if cmd.startswith(\"/\")]\n\n\ndef load_available_skills() -> dict[str, str]:\n    \"\"\"\n    Load all available skills from plugin.\n\n    Returns:\n        Dict mapping directory names to skill names\n        e.g., {'software-architect': 'ctx:architect'}\n    \"\"\"\n    skills_dir = PLUGIN_ROOT / 'skills'\n\n    if not skills_dir.exists():\n        return {}\n\n    skill_map = {}\n\n    for skill_dir in skills_dir.iterdir():\n        if not skill_dir.is_dir():\n            continue\n\n        skill_file = skill_dir / 'SKILL.md'\n        if not skill_file.exists():\n            continue\n\n        # Read skill name from frontmatter\n        try:\n            with open(skill_file) as f:\n                content = f.read()\n                match = re.search(r'^name:\\s*(.+)$', content, re.MULTILINE)\n                if match:\n                    skill_name = match.group(1).strip()\n                    skill_map[skill_dir.name] = skill_name\n        except:\n            continue\n\n    return skill_map\n\n\ndef detect_skill_invocation(prompt: str) -> tuple[bool, str]:\n    \"\"\"\n    Detect if user is trying to invoke a skill explicitly.\n\n    Returns:\n        (is_skill_invocation, attempted_skill_name)\n    \"\"\"\n    patterns = [\n        r'use (?:the )?([a-z\\-:]+) skill',\n        r'with (?:the )?([a-z\\-:]+) skill',\n        r'([a-z\\-:]+) skill to',\n        r'activate (?:the )?([a-z\\-:]+)',\n        r'invoke (?:the )?([a-z\\-:]+)',\n    ]\n\n    for pattern in patterns:\n        match = re.search(pattern, prompt.lower())\n        if match:\n            return True, match.group(1)\n\n    return False, ''\n\n\ndef find_correct_skill_name(attempted_name: str, skill_map: dict[str, str]) -> tuple[str | None, int]:\n    \"\"\"\n    Find correct skill name using fuzzy matching.\n\n    Args:\n        attempted_name: What user tried to use\n        skill_map: Directory \u2192 skill name mapping\n\n    Returns:\n        (correct_name, confidence_score)\n    \"\"\"\n    from rapidfuzz import fuzz\n\n    # Exact directory match\n    if attempted_name in skill_map:\n        return skill_map[attempted_name], 100\n\n    # Exact skill name match (already correct)\n    if attempted_name in skill_map.values():\n        return attempted_name, 100\n\n    # Fuzzy match\n    best_match = None\n    best_score = 0\n\n    for directory, skill_name in skill_map.items():\n        # Directory name\n        score = fuzz.ratio(attempted_name, directory)\n        if score > best_score and score > 70:\n            best_score = score\n            best_match = skill_name\n\n        # Skill name\n        score = fuzz.ratio(attempted_name, skill_name)\n        if score > best_score and score > 70:\n            best_score = score\n            best_match = skill_name\n\n        # Skill name without prefix\n        skill_base = skill_name.replace('ctx:', '')\n        score = fuzz.ratio(attempted_name, skill_base)\n        if score > best_score and score > 70:\n            best_score = score\n            best_match = skill_name\n\n    return best_match, best_score if best_match else 0\n\n\ndef format_suggestion(match: IntentMatch, detection_count: int = 0) -> str:\n    \"\"\"Format detection with directive, actionable phrasing.\"\"\"\n\n    # Get action description\n    action = COMMAND_ACTIONS.get(match.command, \"execute this command\")\n\n    # Build directive message\n    confidence_pct = int(match.confidence * 100)\n\n    # Primary directive message\n    base_msg = (\n        f\"\ud83d\udca1 Type `{match.command}` to {action} ({confidence_pct}% {match.method}\"\n    )\n\n    # Add latency if fast (show performance)\n    if match.latency_ms < 1.0:\n        base_msg += f\", {match.latency_ms:.2f}ms\"\n\n    base_msg += \")\"\n\n    # Get contextual tip\n    tip = get_contextual_tip(match, detection_count)\n\n    if tip:\n        return f\"{base_msg}\\n\ud83d\udca1 {tip}\"\n\n    return base_msg\n\n\ndef format_interactive_suggestion(\n    match: IntentMatch, analysis: dict[str, Any] | None, detection_count: int = 0\n) -> str:\n    \"\"\"\n    Format interactive suggestion with Haiku analysis.\n\n    Args:\n        match: Detected command match\n        analysis: Haiku analysis results (optional)\n        detection_count: Total detections for contextual tips\n\n    Returns:\n        Formatted suggestion message\n    \"\"\"\n    # Get action description\n    action = COMMAND_ACTIONS.get(match.command, \"execute this command\")\n    confidence_pct = int(match.confidence * 100)\n\n    # Base detection message\n    base_msg = f\"\ud83c\udfaf Detected: `{match.command}` ({confidence_pct}% via {match.method})\"\n\n    # Add latency if fast\n    if match.latency_ms < 1.0:\n        base_msg += f\"\\n\u26a1 Detection speed: {match.latency_ms:.2f}ms\"\n\n    # Add Haiku analysis if available\n    if analysis:\n        if not analysis.get(\"is_best_match\", True):\n            alternatives = analysis.get(\"alternatives\", [])\n            if alternatives:\n                base_msg += \"\\n\\n\ud83d\udca1 Better alternatives:\"\n                for alt in alternatives[:3]:\n                    alt_action = COMMAND_ACTIONS.get(alt, \"execute this command\")\n                    base_msg += f\"\\n  \u2022 `{alt}` - {alt_action}\"\n\n        suggestion = analysis.get(\"suggestion\")\n        if suggestion:\n            base_msg += f\"\\n\\n\ud83d\udcac Suggestion: {suggestion}\"\n    else:\n        # Fallback without analysis\n        base_msg += f\"\\n\\n\ud83d\udcdd Action: Type `{match.command}` to {action}\"\n\n    # Get contextual tip\n    tip = get_contextual_tip(match, detection_count)\n    if tip:\n        base_msg += f\"\\n\\n\ud83d\udca1 Tip: {tip}\"\n\n    return base_msg\n\n\ndef detect_git_workflow(prompt: str) -> tuple[bool, str | None]:\n    \"\"\"\n    Detect git workflow patterns in natural language.\n\n    Returns: (is_git_workflow, script_command)\n    \"\"\"\n    prompt_lower = prompt.lower()\n\n    # Git workflow patterns\n    patterns = [\n        # Commit and push\n        (r'\\b(commit|save|push|commit.*push|push.*commit)\\b', '/ctx:git-commit'),\n        # Create PR\n        (r'\\b(create.*pr|make.*pr|open.*pr|pull.*request)\\b', None),  # TODO: /ctx:git-pr\n        # Merge\n        (r'\\b(merge|merge.*branch)\\b', None),  # TODO: /ctx:git-merge\n    ]\n\n    for pattern, command in patterns:\n        if re.search(pattern, prompt_lower):\n            print(f\"DEBUG: Git workflow detected: {pattern} \u2192 {command}\", file=sys.stderr)\n            return True, command\n\n    return False, None\n\n\ndef main():\n    \"\"\"Hook entry point.\"\"\"\n\n    try:\n        # Read hook event from stdin\n        event_json = sys.stdin.read()\n        event = json.loads(event_json)\n\n        prompt = event.get(\"prompt\", \"\")\n\n        # DEBUG: Log what we received\n        print(\n            f\"DEBUG: Promptune hook triggered with prompt: '{prompt}'\", file=sys.stderr\n        )\n\n        # Check if we should process\n        if not should_process(prompt):\n            print(\"DEBUG: Skipping prompt (should_process=False)\", file=sys.stderr)\n            # Pass through unchanged\n            response = {\"continue\": True, \"suppressOutput\": True}\n            print(json.dumps(response))\n            return\n\n        print(\"DEBUG: Processing prompt (should_process=True)\", file=sys.stderr)\n\n        # GIT WORKFLOW DETECTION: Check if user wants git workflow\n        is_git_workflow, git_command = detect_git_workflow(prompt)\n\n        if is_git_workflow and git_command:\n            print(f\"DEBUG: Git workflow detected, suggesting: {git_command}\", file=sys.stderr)\n\n            feedback = f\"\"\"\u26a1 Git Workflow Detected\n\nYour request matches a git workflow pattern.\n\n\ud83d\udca1 **Recommended:** Use the deterministic slash command instead:\n   `{git_command}`\n\n**Benefits:**\n- \u2705 93-97% token reduction (~$0.002 vs ~$0.037-0.086)\n- \u2705 Single command execution\n- \u2705 Deterministic, tested workflow\n- \u2705 Auto-detects remote\n- \u2705 Proper error handling\n\n**Example:**\n```\n{git_command}\n```\n\nContinuing with your original prompt, but consider using the slash command for efficiency.\"\"\"\n\n            response = {\n                \"continue\": True,\n                \"additionalContext\": feedback,\n                \"suppressOutput\": False\n            }\n            print(json.dumps(response))\n            return\n\n        # SKILL DETECTION: Check if user is trying to invoke a skill\n        is_skill_invocation, attempted_skill = detect_skill_invocation(prompt)\n\n        if is_skill_invocation:\n            print(f\"DEBUG: Skill invocation detected: '{attempted_skill}'\", file=sys.stderr)\n\n            # Load available skills\n            skill_map = load_available_skills()\n            print(f\"DEBUG: Loaded {len(skill_map)} skills\", file=sys.stderr)\n\n            # Find correct skill name\n            correct_skill, confidence = find_correct_skill_name(attempted_skill, skill_map)\n\n            if correct_skill and confidence > 70:\n                if attempted_skill != correct_skill:\n                    # Suggest correction\n                    suggestion = f\"\"\"\ud83d\udca1 Skill Name Correction\n\nDetected: Trying to use '{attempted_skill}' skill\nCorrect name: '{correct_skill}' (match confidence: {confidence}%)\n\nUse: {correct_skill}\n\nAvailable skills:\n{chr(10).join(f'  \u2022 {name} (directory: {dir})' for dir, name in skill_map.items())}\n\"\"\"\n                    print(f\"DEBUG: Suggesting skill name correction: {attempted_skill} \u2192 {correct_skill}\", file=sys.stderr)\n\n                    response = {\n                        \"continue\": True,\n                        \"additionalContext\": suggestion,\n                        \"suppressOutput\": False\n                    }\n                    print(json.dumps(response))\n                    return\n                else:\n                    print(f\"DEBUG: Skill name already correct: {correct_skill}\", file=sys.stderr)\n            else:\n                print(f\"DEBUG: No matching skill found for '{attempted_skill}'\", file=sys.stderr)\n\n        # Initialize detector\n        detector = PromptuneDetector()\n\n        # Detect intent\n        match = detector.detect(prompt)\n\n        print(f\"DEBUG: Detection result: {match}\", file=sys.stderr)\n\n        if match is None or match.confidence < 0.7:\n            print(\n                \"DEBUG: No match or low confidence, passing through\", file=sys.stderr\n            )\n            # Clear status line detection (no match)\n            clear_detection_statusline()\n            # No match or low confidence - pass through\n            response = {\"continue\": True, \"suppressOutput\": True}\n            print(json.dumps(response))\n            return\n\n        # Write detection for status line\n        write_detection_for_statusline(match, prompt)\n\n        # Get current detection count for progressive tips\n        detection_count = get_detection_count()\n\n        # Increment counter\n        increment_detection_count()\n\n        print(\n            f\"DEBUG: Command detected (detection #{detection_count + 1})\",\n            file=sys.stderr,\n        )\n\n        # Initialize Haiku engineer for interactive analysis\n        engineer = ClaudeCodeHaikuEngineer()\n        haiku_analysis = None\n        haiku_latency_ms = 0.0\n\n        # Selective triggering: Only run Haiku for low-confidence or fuzzy/semantic matches\n        # High-confidence exact matches (0.95+) are reliable and don't need Haiku validation\n        should_run_haiku = match.confidence < 0.95 or match.method in ['fuzzy', 'semantic']\n\n        if should_run_haiku:\n            print(f\"DEBUG: Triggering Haiku analysis (confidence={match.confidence:.2f}, method={match.method})\", file=sys.stderr)\n        else:\n            print(f\"DEBUG: Skipping Haiku analysis (high-confidence {match.method} match: {match.confidence:.2f})\", file=sys.stderr)\n\n        # Try to get Haiku analysis for better suggestions (only if needed)\n        if should_run_haiku and engineer.is_available():\n            print(f\"DEBUG: Running Haiku analysis...\", file=sys.stderr)\n            available_commands = load_available_commands()\n\n            # Track Haiku analysis latency\n            import time\n\n            haiku_start = time.perf_counter()\n\n            haiku_analysis = engineer.analyze_and_enhance(\n                prompt=prompt,\n                detected_command=match.command,\n                confidence=match.confidence,\n                available_commands=available_commands,\n                timeout=30,\n            )\n\n            haiku_latency_ms = (time.perf_counter() - haiku_start) * 1000\n\n            if haiku_analysis:\n                print(\n                    f\"DEBUG: Haiku analysis: {json.dumps(haiku_analysis)}\",\n                    file=sys.stderr,\n                )\n            else:\n                print(f\"DEBUG: Haiku analysis failed or timed out\", file=sys.stderr)\n        elif not should_run_haiku:\n            print(f\"DEBUG: Haiku analysis skipped (selective triggering)\", file=sys.stderr)\n        else:\n            print(\n                \"DEBUG: Claude Code CLI not available, skipping Haiku analysis\",\n                file=sys.stderr,\n            )\n\n        # AUGMENT MODE: Modify prompt with skill/command suggestion for reliability\n        print(\"DEBUG: Augmenting prompt for Claude\", file=sys.stderr)\n\n        # Create augmented prompt with skill suggestion\n        augmented_prompt = create_skill_augmented_prompt(match, prompt)\n\n        # Format interactive suggestion with Haiku analysis (if available)\n        interactive_msg = format_interactive_suggestion(\n            match, haiku_analysis, detection_count\n        )\n\n        # Determine the best command to use\n        # If Haiku suggests alternatives and it's not the best match, use the first alternative\n        best_command = match.command\n        correction_accepted = False\n\n        if haiku_analysis and not haiku_analysis.get(\"is_best_match\", True):\n            alternatives = haiku_analysis.get(\"alternatives\", [])\n            if alternatives:\n                best_command = alternatives[0]\n                correction_accepted = True\n                print(\n                    f\"DEBUG: Haiku suggests using {best_command} instead of {match.command}\",\n                    file=sys.stderr,\n                )\n\n        # Log correction to observability DB\n        if haiku_analysis:\n            try:\n                db = ObservabilityDB(\".promptune/observability.db\")\n\n                # Estimate token counts (rough approximation)\n                # Haiku prompt is ~150 tokens + command list + user prompt\n                prompt_tokens = (\n                    150 + len(prompt.split()) + len(load_available_commands()) * 5\n                )\n                # Response is typically ~50-100 tokens\n                completion_tokens = (\n                    50 + len(str(haiku_analysis.get(\"suggestion\", \"\"))) // 4\n                )\n\n                db.log_correction(\n                    original_command=match.command,\n                    corrected_command=best_command,\n                    original_confidence=match.confidence,\n                    correction_accepted=correction_accepted,\n                    model_name=\"haiku-4-5\",\n                    reasoning=haiku_analysis.get(\"suggestion\", \"\"),\n                    prompt_tokens=prompt_tokens,\n                    completion_tokens=completion_tokens,\n                    latency_ms=haiku_latency_ms,\n                    session_id=event.get(\"session_id\", \"\"),\n                    prompt_preview=prompt[:100],\n                )\n                print(\n                    f\"DEBUG: Logged correction to observability DB (accepted={correction_accepted})\",\n                    file=sys.stderr,\n                )\n            except Exception as e:\n                print(f\"DEBUG: Failed to log correction: {e}\", file=sys.stderr)\n\n        # Create augmented prompt with the best command (potentially corrected by Haiku)\n        if best_command != match.command:\n            # Use Haiku's suggested command\n            action = COMMAND_ACTIONS.get(best_command, \"complete this request\")\n            if best_command in SKILL_MAPPING:\n                skill_name = SKILL_MAPPING[best_command]\n                augmented_prompt = f\"{prompt}. You can use your {skill_name} skill to help with this task.\"\n            else:\n                augmented_prompt = (\n                    f\"{prompt}. Please use the {best_command} command to {action}.\"\n                )\n        else:\n            # Use original detection\n            augmented_prompt = create_skill_augmented_prompt(match, prompt)\n\n        # SUGGEST-ONLY MODE: Show detection but let user decide\n        # Don't auto-execute - just provide helpful suggestion\n        print(\n            f\"DEBUG: Using suggest-only mode - showing suggestion for {best_command}\",\n            file=sys.stderr,\n        )\n\n        response = {\n            \"continue\": True,\n            \"additionalContext\": interactive_msg,\n            \"suppressOutput\": False\n        }\n\n        print(f\"DEBUG: Response: {json.dumps(response)}\", file=sys.stderr)\n        print(json.dumps(response))\n\n    except Exception as e:\n        # Log error but don't block Claude\n        import traceback\n\n        print(f\"Promptune error: {e}\", file=sys.stderr)\n        print(f\"DEBUG: Traceback: {traceback.format_exc()}\", file=sys.stderr)\n        response = {\"continue\": True, \"suppressOutput\": True}\n        print(json.dumps(response))\n\n\nif __name__ == \"__main__\":\n    main()\n",
			"structuredPatch": [
				{
					"oldStart": 38,
					"oldLines": 9,
					"newStart": 38,
					"newLines": 17,
					"lines": [
						" from keyword_matcher_v2 import IntentMatch, KeywordMatcherV2 as KeywordMatcher",
						" from model2vec_matcher import Model2VecMatcher",
						" from observability_db import ObservabilityDB",
						"-from semantic_router_matcher import SemanticRouterMatcher",
						" ",
						"+# Gracefully handle semantic_router import errors (logging issues, etc.)",
						"+try:",
						"+    from semantic_router_matcher import SemanticRouterMatcher",
						"+    SEMANTIC_ROUTER_AVAILABLE = True",
						"+except Exception as e:",
						"+    print(f\"Warning: semantic_router unavailable: {e}\", file=sys.stderr)",
						"+    SemanticRouterMatcher = None",
						"+    SEMANTIC_ROUTER_AVAILABLE = False",
						" ",
						"+",
						" class PromptuneDetector:",
						"     \"\"\"",
						"     3-tier intent detection cascade."
					]
				}
			],
			"userModified": false,
			"replaceAll": false
		},
		"tool_use_id": "toolu_01NgRy4j1FAHpb8jhm3BWDB8"
	},
	{
		"session_id": "9202596c-d296-440a-b61f-1fca4bc5b4ea",
		"transcript_path": "/Users/promptune/.claude/projects/-Users-promptune-DevProjects-promptune/9202596c-d296-440a-b61f-1fca4bc5b4ea.jsonl",
		"cwd": "/Users/promptune/DevProjects/promptune",
		"permission_mode": "acceptEdits",
		"hook_event_name": "PostToolUse",
		"tool_name": "Read",
		"tool_input": {
			"file_path": "/Users/promptune/DevProjects/promptune/hooks/user_prompt_submit.py",
			"offset": 50,
			"limit": 50
		},
		"tool_response": {
			"type": "text",
			"file": {
				"filePath": "/Users/promptune/DevProjects/promptune/hooks/user_prompt_submit.py",
				"content": "\n\nclass PromptuneDetector:\n    \"\"\"\n    3-tier intent detection cascade.\n\n    Uses your existing matchers in order of speed:\n    1. KeywordMatcher (always fast)\n    2. Model2VecMatcher (if available)\n    3. SemanticRouterMatcher (if API key available)\n    \"\"\"\n\n    def __init__(self):\n        self._keyword = None\n        self._model2vec = None\n        self._semantic = None\n\n    def _get_keyword(self):\n        if self._keyword is None:\n            self._keyword = KeywordMatcher()\n        return self._keyword\n\n    def _get_model2vec(self):\n        if self._model2vec is None:\n            m = Model2VecMatcher()\n            self._model2vec = m if m.is_available() else None\n        return self._model2vec\n\n    def _get_semantic(self):\n        if self._semantic is None:\n            m = SemanticRouterMatcher()\n            self._semantic = m if m.is_available() else None\n        return self._semantic\n\n    def detect(self, text: str) -> IntentMatch | None:\n        \"\"\"Detect intent using 3-tier cascade.\"\"\"\n\n        # Tier 1: Keyword (always available)\n        result = self._get_keyword().match(text)\n        if result:\n            return result\n\n        # Tier 2: Model2Vec\n        m2v = self._get_model2vec()\n        if m2v:\n            result = m2v.match(text)\n            if result:\n                return result\n\n        # Tier 3: Semantic Router",
				"numLines": 50,
				"startLine": 50,
				"totalLines": 923
			}
		},
		"tool_use_id": "toolu_01XnXapBDhFULtWctVXYanuM"
	},
	{
		"session_id": "9202596c-d296-440a-b61f-1fca4bc5b4ea",
		"transcript_path": "/Users/promptune/.claude/projects/-Users-promptune-DevProjects-promptune/9202596c-d296-440a-b61f-1fca4bc5b4ea.jsonl",
		"cwd": "/Users/promptune/DevProjects/promptune",
		"permission_mode": "acceptEdits",
		"hook_event_name": "PostToolUse",
		"tool_name": "Edit",
		"tool_input": {
			"file_path": "/Users/promptune/DevProjects/promptune/hooks/user_prompt_submit.py",
			"old_string": "    def _get_semantic(self):\n        if self._semantic is None:\n            m = SemanticRouterMatcher()\n            self._semantic = m if m.is_available() else None\n        return self._semantic",
			"new_string": "    def _get_semantic(self):\n        if self._semantic is None and SEMANTIC_ROUTER_AVAILABLE:\n            m = SemanticRouterMatcher()\n            self._semantic = m if m.is_available() else None\n        return self._semantic"
		},
		"tool_response": {
			"filePath": "/Users/promptune/DevProjects/promptune/hooks/user_prompt_submit.py",
			"oldString": "    def _get_semantic(self):\n        if self._semantic is None:\n            m = SemanticRouterMatcher()\n            self._semantic = m if m.is_available() else None\n        return self._semantic",
			"newString": "    def _get_semantic(self):\n        if self._semantic is None and SEMANTIC_ROUTER_AVAILABLE:\n            m = SemanticRouterMatcher()\n            self._semantic = m if m.is_available() else None\n        return self._semantic",
			"originalFile": "#!/usr/bin/env -S uv run --script\n# /// script\n# requires-python = \">=3.10\"\n# dependencies = [\n#     \"model2vec>=0.3.0\",\n#     \"semantic-router>=0.1.0\",\n#     \"numpy>=1.24.0\",\n#     \"rapidfuzz>=3.0.0\"\n# ]\n# ///\n\"\"\"\nPromptune UserPromptSubmit Hook\n\nDetects slash commands from natural language prompts using 3-tier cascade:\n1. Keyword matching (0.02ms, 60% coverage)\n2. Model2Vec embeddings (0.2ms, 30% coverage)\n3. Semantic Router (50ms, 10% coverage)\n\nUses Claude Code headless mode for interactive prompt analysis and suggestions.\n\nHook Protocol:\n- Input: JSON via stdin with {\"prompt\": \"...\", \"session_id\": \"...\"}\n- Output: JSON via stdout with {\"continue\": true, \"feedback\": \"...\"}\n\"\"\"\n\nimport json\nimport subprocess\nimport sys\nimport re\nfrom pathlib import Path\nfrom typing import Any\n\n# Add lib directory to Python path\nPLUGIN_ROOT = Path(__file__).parent.parent\nsys.path.insert(0, str(PLUGIN_ROOT / \"lib\"))\n\n# Import matchers (now using RapidFuzz-based keyword matcher v2!)\nfrom keyword_matcher_v2 import IntentMatch, KeywordMatcherV2 as KeywordMatcher\nfrom model2vec_matcher import Model2VecMatcher\nfrom observability_db import ObservabilityDB\n\n# Gracefully handle semantic_router import errors (logging issues, etc.)\ntry:\n    from semantic_router_matcher import SemanticRouterMatcher\n    SEMANTIC_ROUTER_AVAILABLE = True\nexcept Exception as e:\n    print(f\"Warning: semantic_router unavailable: {e}\", file=sys.stderr)\n    SemanticRouterMatcher = None\n    SEMANTIC_ROUTER_AVAILABLE = False\n\n\nclass PromptuneDetector:\n    \"\"\"\n    3-tier intent detection cascade.\n\n    Uses your existing matchers in order of speed:\n    1. KeywordMatcher (always fast)\n    2. Model2VecMatcher (if available)\n    3. SemanticRouterMatcher (if API key available)\n    \"\"\"\n\n    def __init__(self):\n        self._keyword = None\n        self._model2vec = None\n        self._semantic = None\n\n    def _get_keyword(self):\n        if self._keyword is None:\n            self._keyword = KeywordMatcher()\n        return self._keyword\n\n    def _get_model2vec(self):\n        if self._model2vec is None:\n            m = Model2VecMatcher()\n            self._model2vec = m if m.is_available() else None\n        return self._model2vec\n\n    def _get_semantic(self):\n        if self._semantic is None:\n            m = SemanticRouterMatcher()\n            self._semantic = m if m.is_available() else None\n        return self._semantic\n\n    def detect(self, text: str) -> IntentMatch | None:\n        \"\"\"Detect intent using 3-tier cascade.\"\"\"\n\n        # Tier 1: Keyword (always available)\n        result = self._get_keyword().match(text)\n        if result:\n            return result\n\n        # Tier 2: Model2Vec\n        m2v = self._get_model2vec()\n        if m2v:\n            result = m2v.match(text)\n            if result:\n                return result\n\n        # Tier 3: Semantic Router\n        sem = self._get_semantic()\n        if sem:\n            result = sem.match(text)\n            if result:\n                return result\n\n        return None\n\n\nclass ClaudeCodeHaikuEngineer:\n    \"\"\"\n    Uses Claude Code headless mode to analyze prompts and provide interactive suggestions.\n\n    Benefits:\n    - No separate API key needed (uses existing Claude Code auth)\n    - Integrated billing with Claude Code\n    - Fast Haiku model for cost optimization\n    - Interactive blocking mode for user feedback\n    \"\"\"\n\n    def __init__(self):\n        self._claude_available = None\n\n    def is_available(self) -> bool:\n        \"\"\"Check if Claude Code CLI is available.\"\"\"\n        if self._claude_available is None:\n            try:\n                result = subprocess.run(\n                    [\"claude\", \"--version\"], capture_output=True, text=True, timeout=2\n                )\n                self._claude_available = result.returncode == 0\n            except (FileNotFoundError, subprocess.TimeoutExpired):\n                self._claude_available = False\n\n        return self._claude_available\n\n    def analyze_and_enhance(\n        self,\n        prompt: str,\n        detected_command: str,\n        confidence: float,\n        available_commands: list[str],\n        timeout: int = 30,\n    ) -> dict[str, Any] | None:\n        \"\"\"\n        Analyze prompt using Claude Code headless mode and suggest enhancements.\n\n        Args:\n            prompt: User's original prompt\n            detected_command: Command detected by cascade\n            confidence: Detection confidence (0-1)\n            available_commands: List of all available commands\n            timeout: Timeout in seconds\n\n        Returns:\n            Dict with analysis results or None if unavailable/failed\n        \"\"\"\n        if not self.is_available():\n            return None\n\n        # Build analysis prompt for Haiku\n        analysis_prompt = f\"\"\"You are a prompt enhancement assistant for Promptune, a Claude Code plugin.\n\nUSER'S PROMPT: \"{prompt}\"\n\nDETECTED COMMAND: {detected_command}\nDETECTION CONFIDENCE: {confidence:.0%}\n\nAVAILABLE ALTERNATIVES:\n{chr(10).join(f\"- {cmd}\" for cmd in available_commands[:10])}\n\nTASK: Analyze the user's prompt and provide:\n1. Whether the detected command is the best match (true/false)\n2. Alternative commands if better matches exist\n3. A brief, helpful suggestion for the user\n\nRESPONSE FORMAT (JSON):\n{{\n  \"is_best_match\": true/false,\n  \"alternatives\": [\"command1\", \"command2\"],\n  \"suggestion\": \"Brief suggestion text\"\n}}\n\nBe concise. Focus on actionability.\"\"\"\n\n        try:\n            # Call Claude Code headless with Haiku model\n            cmd = [\n                \"claude\",\n                \"--model\",\n                \"claude-haiku-4-5\",\n                \"-p\",\n                analysis_prompt,\n                \"--output-format\",\n                \"json\",\n                \"--allowedTools\",\n                \"\",  # No tools needed for this analysis\n            ]\n\n            result = subprocess.run(\n                cmd, capture_output=True, text=True, timeout=timeout\n            )\n\n            if result.returncode != 0:\n                print(f\"DEBUG: Claude Code error: {result.stderr}\", file=sys.stderr)\n                return None\n\n            # Parse Claude's response - it's nested in a wrapper object\n            claude_response = json.loads(result.stdout)\n\n            # Extract the actual result (may be nested in \"result\" field)\n            if \"result\" in claude_response:\n                result_text = claude_response[\"result\"]\n                # Result may contain JSON in markdown code blocks\n                if \"```json\" in result_text:\n                    # Extract JSON from markdown code block\n                    json_start = result_text.find(\"```json\") + 7\n                    json_end = result_text.find(\"```\", json_start)\n                    result_text = result_text[json_start:json_end].strip()\n\n                # Parse the extracted JSON\n                analysis = json.loads(result_text)\n                return analysis\n            else:\n                # If no \"result\" field, assume the whole response is the analysis\n                return claude_response\n\n        except subprocess.TimeoutExpired:\n            print(f\"DEBUG: Claude Code timeout after {timeout}s\", file=sys.stderr)\n            return None\n        except json.JSONDecodeError as e:\n            print(f\"DEBUG: Failed to parse Claude response: {e}\", file=sys.stderr)\n            print(f\"DEBUG: Raw output: {result.stdout[:200]}\", file=sys.stderr)\n            return None\n        except Exception as e:\n            print(f\"DEBUG: Haiku engineer error: {e}\", file=sys.stderr)\n            return None\n\n\ndef should_process(prompt: str) -> bool:\n    \"\"\"Check if prompt needs intent detection.\"\"\"\n    if not prompt or not prompt.strip():\n        return False\n\n    # Skip if already a command\n    if prompt.strip().startswith(\"/\"):\n        return False\n\n    # Skip internal Haiku analysis prompts (prevent feedback loop)\n    if prompt.startswith(\"You are a prompt enhancement assistant\"):\n        return False\n\n    # Skip if too short\n    if len(prompt.strip().split()) < 3:\n        return False\n\n    return True\n\n\ndef write_detection_for_statusline(match: IntentMatch, prompt: str):\n    \"\"\"Write detection data to observability DB for status line to read.\"\"\"\n    try:\n        db = ObservabilityDB(\".promptune/observability.db\")\n        db.set_detection(\n            command=match.command,\n            confidence=match.confidence,\n            method=match.method,\n            prompt_preview=prompt[:60] + (\"...\" if len(prompt) > 60 else \"\"),\n            latency_ms=match.latency_ms,\n        )\n\n        # Also log matcher performance\n        db.log_matcher_performance(match.method, match.latency_ms, success=True)\n\n        print(\n            f\"DEBUG: Wrote detection to observability DB: {match.command} ({match.confidence:.0%} {match.method})\",\n            file=sys.stderr,\n        )\n    except Exception as e:\n        # Don't fail hook if observability write fails\n        print(f\"DEBUG: Failed to write to observability DB: {e}\", file=sys.stderr)\n        # Also log the error\n        try:\n            db = ObservabilityDB(\".promptune/observability.db\")\n            db.log_error(\"user_prompt_submit\", type(e).__name__, str(e))\n        except:\n            pass\n\n\ndef clear_detection_statusline():\n    \"\"\"Clear status line detection (no match found).\"\"\"\n    try:\n        db = ObservabilityDB(\".promptune/observability.db\")\n        db.clear_detection()\n        print(\"DEBUG: Cleared detection from observability DB\", file=sys.stderr)\n    except Exception as e:\n        print(\n            f\"DEBUG: Failed to clear detection from observability DB: {e}\",\n            file=sys.stderr,\n        )\n\n\ndef get_detection_count() -> int:\n    \"\"\"Get total number of detections for progressive tips.\"\"\"\n    try:\n        db = ObservabilityDB(\".promptune/observability.db\")\n        stats = db.get_stats()\n        return stats.get(\"detections\", {}).get(\"total\", 0)\n    except:\n        pass\n    return 0\n\n\ndef increment_detection_count():\n    \"\"\"Increment detection counter for progressive disclosure.\"\"\"\n    try:\n        data_dir = Path.home() / \".claude\" / \"plugins\" / \"promptune\" / \"data\"\n        data_dir.mkdir(parents=True, exist_ok=True)\n        stats_file = data_dir / \"detection_stats.json\"\n\n        stats = {\"total_detections\": 0, \"by_method\": {}, \"by_command\": {}}\n        if stats_file.exists():\n            with open(stats_file) as f:\n                stats = json.load(f)\n\n        stats[\"total_detections\"] = stats.get(\"total_detections\", 0) + 1\n\n        with open(stats_file, \"w\") as f:\n            json.dump(stats, f, indent=2)\n    except:\n        pass  # Don't fail hook if stats tracking fails\n\n\n# Command action descriptions for directive feedback\nCOMMAND_ACTIONS = {\n    # Promptune commands\n    \"/ctx:design\": \"design system architecture with structured workflow\",\n    \"/ctx:research\": \"get fast answers using 3 parallel agents\",\n    \"/ctx:plan\": \"create parallel development plans\",\n    \"/ctx:execute\": \"run tasks in parallel worktrees\",\n    \"/ctx:status\": \"monitor parallel task progress\",\n    \"/ctx:cleanup\": \"clean up completed worktrees\",\n    \"/ctx:help\": \"see example-first command guide\",\n    \"/ctx:configure\": \"enable persistent status bar display\",\n    \"/ctx:stats\": \"see your time & cost savings\",\n    \"/ctx:verify\": \"verify and execute detected command with confirmation\",\n    # Skill-only detections (no commands)\n    \"skill:ctx:performance\": \"analyze and optimize parallel workflow performance\",\n    \"skill:ctx:parallel-expert\": \"get guidance on parallelizing tasks effectively\",\n    \"skill:ctx:help\": \"discover Promptune features and capabilities\",\n    \"skill:ctx:worktree\": \"troubleshoot git worktree issues and conflicts\",\n}\n\n# Skill mapping for reliable Claude execution\n# Maps slash commands AND skill detections to skill names\n# Skills are auto-discovered by Claude Code from: promptune/skills/*/SKILL.md\nSKILL_MAPPING = {\n    # Commands with skills\n    \"/ctx:design\": \"ctx:architect\",  # Plugin skill: skills/software-architect\n    \"/ctx:research\": \"ctx:researcher\",  # Plugin skill: skills/researcher\n    # Skills without commands (direct skill suggestions)\n    \"skill:ctx:performance\": \"ctx:performance\",\n    \"skill:ctx:parallel-expert\": \"ctx:parallel-expert\",\n    \"skill:ctx:help\": \"ctx:help\",\n    \"skill:ctx:worktree\": \"ctx:worktree\",\n    # Note: /ctx:plan and /ctx:execute are commands, not skills\n    # They execute workflows directly rather than providing guidance\n}\n\n\ndef create_skill_augmented_prompt(match: IntentMatch, original_prompt: str) -> str:\n    \"\"\"\n    Augment prompt with skill suggestion for more reliable execution.\n\n    Evidence: Skills are invoked more reliably than slash commands because\n    they use Claude's native Skill tool (structured, type-safe) vs text expansion.\n\n    Args:\n        match: Detected command and confidence\n        original_prompt: User's original prompt text\n\n    Returns:\n        Augmented prompt that guides Claude to use skill or command\n    \"\"\"\n    if match.command in SKILL_MAPPING:\n        skill_name = SKILL_MAPPING[match.command]\n        # Strong directive: \"You can use your X skill\"\n        return f\"{original_prompt}. You can use your {skill_name} skill to help with this task.\"\n    else:\n        # For commands without skills, use directive language\n        action = COMMAND_ACTIONS.get(match.command, \"complete this request\")\n        return f\"{original_prompt}. Please use the {match.command} command to {action}.\"\n\n\ndef get_contextual_tip(match: IntentMatch, detection_count: int) -> str:\n    \"\"\"Generate directive contextual tip based on usage patterns.\"\"\"\n\n    # First-time users (1-3 detections)\n    if detection_count <= 3:\n        return \"New user? Type `/ctx:help` to see all commands with examples\"\n\n    # Early users (4-10 detections) - promote status bar\n    elif detection_count <= 10:\n        return \"Enable persistent detection: Type `/ctx:configure` to set up status bar\"\n\n    # Experienced users (11-20) - promote advanced features\n    elif detection_count <= 20:\n        if match.command.startswith(\"/ctx:\"):\n            return \"Want parallel workflows? Type `/ctx:plan` to work on multiple tasks simultaneously\"\n        return f\"Blazing fast: {match.latency_ms:.2f}ms detection. Type `/ctx:stats` to see all metrics\"\n\n    # Power users (21+) - occasional celebration\n    else:\n        if detection_count % 10 == 0:  # Every 10th detection\n            return f\"\ud83c\udf89 {detection_count} detections! Type `/ctx:stats` to see your time & cost savings\"\n        return None  # No tip for most interactions\n\n\ndef load_available_commands() -> list[str]:\n    \"\"\"Load list of all available commands for Claude Code.\"\"\"\n    # Return all commands from COMMAND_ACTIONS\n    return [cmd for cmd in COMMAND_ACTIONS if cmd.startswith(\"/\")]\n\n\ndef load_available_skills() -> dict[str, str]:\n    \"\"\"\n    Load all available skills from plugin.\n\n    Returns:\n        Dict mapping directory names to skill names\n        e.g., {'software-architect': 'ctx:architect'}\n    \"\"\"\n    skills_dir = PLUGIN_ROOT / 'skills'\n\n    if not skills_dir.exists():\n        return {}\n\n    skill_map = {}\n\n    for skill_dir in skills_dir.iterdir():\n        if not skill_dir.is_dir():\n            continue\n\n        skill_file = skill_dir / 'SKILL.md'\n        if not skill_file.exists():\n            continue\n\n        # Read skill name from frontmatter\n        try:\n            with open(skill_file) as f:\n                content = f.read()\n                match = re.search(r'^name:\\s*(.+)$', content, re.MULTILINE)\n                if match:\n                    skill_name = match.group(1).strip()\n                    skill_map[skill_dir.name] = skill_name\n        except:\n            continue\n\n    return skill_map\n\n\ndef detect_skill_invocation(prompt: str) -> tuple[bool, str]:\n    \"\"\"\n    Detect if user is trying to invoke a skill explicitly.\n\n    Returns:\n        (is_skill_invocation, attempted_skill_name)\n    \"\"\"\n    patterns = [\n        r'use (?:the )?([a-z\\-:]+) skill',\n        r'with (?:the )?([a-z\\-:]+) skill',\n        r'([a-z\\-:]+) skill to',\n        r'activate (?:the )?([a-z\\-:]+)',\n        r'invoke (?:the )?([a-z\\-:]+)',\n    ]\n\n    for pattern in patterns:\n        match = re.search(pattern, prompt.lower())\n        if match:\n            return True, match.group(1)\n\n    return False, ''\n\n\ndef find_correct_skill_name(attempted_name: str, skill_map: dict[str, str]) -> tuple[str | None, int]:\n    \"\"\"\n    Find correct skill name using fuzzy matching.\n\n    Args:\n        attempted_name: What user tried to use\n        skill_map: Directory \u2192 skill name mapping\n\n    Returns:\n        (correct_name, confidence_score)\n    \"\"\"\n    from rapidfuzz import fuzz\n\n    # Exact directory match\n    if attempted_name in skill_map:\n        return skill_map[attempted_name], 100\n\n    # Exact skill name match (already correct)\n    if attempted_name in skill_map.values():\n        return attempted_name, 100\n\n    # Fuzzy match\n    best_match = None\n    best_score = 0\n\n    for directory, skill_name in skill_map.items():\n        # Directory name\n        score = fuzz.ratio(attempted_name, directory)\n        if score > best_score and score > 70:\n            best_score = score\n            best_match = skill_name\n\n        # Skill name\n        score = fuzz.ratio(attempted_name, skill_name)\n        if score > best_score and score > 70:\n            best_score = score\n            best_match = skill_name\n\n        # Skill name without prefix\n        skill_base = skill_name.replace('ctx:', '')\n        score = fuzz.ratio(attempted_name, skill_base)\n        if score > best_score and score > 70:\n            best_score = score\n            best_match = skill_name\n\n    return best_match, best_score if best_match else 0\n\n\ndef format_suggestion(match: IntentMatch, detection_count: int = 0) -> str:\n    \"\"\"Format detection with directive, actionable phrasing.\"\"\"\n\n    # Get action description\n    action = COMMAND_ACTIONS.get(match.command, \"execute this command\")\n\n    # Build directive message\n    confidence_pct = int(match.confidence * 100)\n\n    # Primary directive message\n    base_msg = (\n        f\"\ud83d\udca1 Type `{match.command}` to {action} ({confidence_pct}% {match.method}\"\n    )\n\n    # Add latency if fast (show performance)\n    if match.latency_ms < 1.0:\n        base_msg += f\", {match.latency_ms:.2f}ms\"\n\n    base_msg += \")\"\n\n    # Get contextual tip\n    tip = get_contextual_tip(match, detection_count)\n\n    if tip:\n        return f\"{base_msg}\\n\ud83d\udca1 {tip}\"\n\n    return base_msg\n\n\ndef format_interactive_suggestion(\n    match: IntentMatch, analysis: dict[str, Any] | None, detection_count: int = 0\n) -> str:\n    \"\"\"\n    Format interactive suggestion with Haiku analysis.\n\n    Args:\n        match: Detected command match\n        analysis: Haiku analysis results (optional)\n        detection_count: Total detections for contextual tips\n\n    Returns:\n        Formatted suggestion message\n    \"\"\"\n    # Get action description\n    action = COMMAND_ACTIONS.get(match.command, \"execute this command\")\n    confidence_pct = int(match.confidence * 100)\n\n    # Base detection message\n    base_msg = f\"\ud83c\udfaf Detected: `{match.command}` ({confidence_pct}% via {match.method})\"\n\n    # Add latency if fast\n    if match.latency_ms < 1.0:\n        base_msg += f\"\\n\u26a1 Detection speed: {match.latency_ms:.2f}ms\"\n\n    # Add Haiku analysis if available\n    if analysis:\n        if not analysis.get(\"is_best_match\", True):\n            alternatives = analysis.get(\"alternatives\", [])\n            if alternatives:\n                base_msg += \"\\n\\n\ud83d\udca1 Better alternatives:\"\n                for alt in alternatives[:3]:\n                    alt_action = COMMAND_ACTIONS.get(alt, \"execute this command\")\n                    base_msg += f\"\\n  \u2022 `{alt}` - {alt_action}\"\n\n        suggestion = analysis.get(\"suggestion\")\n        if suggestion:\n            base_msg += f\"\\n\\n\ud83d\udcac Suggestion: {suggestion}\"\n    else:\n        # Fallback without analysis\n        base_msg += f\"\\n\\n\ud83d\udcdd Action: Type `{match.command}` to {action}\"\n\n    # Get contextual tip\n    tip = get_contextual_tip(match, detection_count)\n    if tip:\n        base_msg += f\"\\n\\n\ud83d\udca1 Tip: {tip}\"\n\n    return base_msg\n\n\ndef detect_git_workflow(prompt: str) -> tuple[bool, str | None]:\n    \"\"\"\n    Detect git workflow patterns in natural language.\n\n    Returns: (is_git_workflow, script_command)\n    \"\"\"\n    prompt_lower = prompt.lower()\n\n    # Git workflow patterns\n    patterns = [\n        # Commit and push\n        (r'\\b(commit|save|push|commit.*push|push.*commit)\\b', '/ctx:git-commit'),\n        # Create PR\n        (r'\\b(create.*pr|make.*pr|open.*pr|pull.*request)\\b', None),  # TODO: /ctx:git-pr\n        # Merge\n        (r'\\b(merge|merge.*branch)\\b', None),  # TODO: /ctx:git-merge\n    ]\n\n    for pattern, command in patterns:\n        if re.search(pattern, prompt_lower):\n            print(f\"DEBUG: Git workflow detected: {pattern} \u2192 {command}\", file=sys.stderr)\n            return True, command\n\n    return False, None\n\n\ndef main():\n    \"\"\"Hook entry point.\"\"\"\n\n    try:\n        # Read hook event from stdin\n        event_json = sys.stdin.read()\n        event = json.loads(event_json)\n\n        prompt = event.get(\"prompt\", \"\")\n\n        # DEBUG: Log what we received\n        print(\n            f\"DEBUG: Promptune hook triggered with prompt: '{prompt}'\", file=sys.stderr\n        )\n\n        # Check if we should process\n        if not should_process(prompt):\n            print(\"DEBUG: Skipping prompt (should_process=False)\", file=sys.stderr)\n            # Pass through unchanged\n            response = {\"continue\": True, \"suppressOutput\": True}\n            print(json.dumps(response))\n            return\n\n        print(\"DEBUG: Processing prompt (should_process=True)\", file=sys.stderr)\n\n        # GIT WORKFLOW DETECTION: Check if user wants git workflow\n        is_git_workflow, git_command = detect_git_workflow(prompt)\n\n        if is_git_workflow and git_command:\n            print(f\"DEBUG: Git workflow detected, suggesting: {git_command}\", file=sys.stderr)\n\n            feedback = f\"\"\"\u26a1 Git Workflow Detected\n\nYour request matches a git workflow pattern.\n\n\ud83d\udca1 **Recommended:** Use the deterministic slash command instead:\n   `{git_command}`\n\n**Benefits:**\n- \u2705 93-97% token reduction (~$0.002 vs ~$0.037-0.086)\n- \u2705 Single command execution\n- \u2705 Deterministic, tested workflow\n- \u2705 Auto-detects remote\n- \u2705 Proper error handling\n\n**Example:**\n```\n{git_command}\n```\n\nContinuing with your original prompt, but consider using the slash command for efficiency.\"\"\"\n\n            response = {\n                \"continue\": True,\n                \"additionalContext\": feedback,\n                \"suppressOutput\": False\n            }\n            print(json.dumps(response))\n            return\n\n        # SKILL DETECTION: Check if user is trying to invoke a skill\n        is_skill_invocation, attempted_skill = detect_skill_invocation(prompt)\n\n        if is_skill_invocation:\n            print(f\"DEBUG: Skill invocation detected: '{attempted_skill}'\", file=sys.stderr)\n\n            # Load available skills\n            skill_map = load_available_skills()\n            print(f\"DEBUG: Loaded {len(skill_map)} skills\", file=sys.stderr)\n\n            # Find correct skill name\n            correct_skill, confidence = find_correct_skill_name(attempted_skill, skill_map)\n\n            if correct_skill and confidence > 70:\n                if attempted_skill != correct_skill:\n                    # Suggest correction\n                    suggestion = f\"\"\"\ud83d\udca1 Skill Name Correction\n\nDetected: Trying to use '{attempted_skill}' skill\nCorrect name: '{correct_skill}' (match confidence: {confidence}%)\n\nUse: {correct_skill}\n\nAvailable skills:\n{chr(10).join(f'  \u2022 {name} (directory: {dir})' for dir, name in skill_map.items())}\n\"\"\"\n                    print(f\"DEBUG: Suggesting skill name correction: {attempted_skill} \u2192 {correct_skill}\", file=sys.stderr)\n\n                    response = {\n                        \"continue\": True,\n                        \"additionalContext\": suggestion,\n                        \"suppressOutput\": False\n                    }\n                    print(json.dumps(response))\n                    return\n                else:\n                    print(f\"DEBUG: Skill name already correct: {correct_skill}\", file=sys.stderr)\n            else:\n                print(f\"DEBUG: No matching skill found for '{attempted_skill}'\", file=sys.stderr)\n\n        # Initialize detector\n        detector = PromptuneDetector()\n\n        # Detect intent\n        match = detector.detect(prompt)\n\n        print(f\"DEBUG: Detection result: {match}\", file=sys.stderr)\n\n        if match is None or match.confidence < 0.7:\n            print(\n                \"DEBUG: No match or low confidence, passing through\", file=sys.stderr\n            )\n            # Clear status line detection (no match)\n            clear_detection_statusline()\n            # No match or low confidence - pass through\n            response = {\"continue\": True, \"suppressOutput\": True}\n            print(json.dumps(response))\n            return\n\n        # Write detection for status line\n        write_detection_for_statusline(match, prompt)\n\n        # Get current detection count for progressive tips\n        detection_count = get_detection_count()\n\n        # Increment counter\n        increment_detection_count()\n\n        print(\n            f\"DEBUG: Command detected (detection #{detection_count + 1})\",\n            file=sys.stderr,\n        )\n\n        # Initialize Haiku engineer for interactive analysis\n        engineer = ClaudeCodeHaikuEngineer()\n        haiku_analysis = None\n        haiku_latency_ms = 0.0\n\n        # Selective triggering: Only run Haiku for low-confidence or fuzzy/semantic matches\n        # High-confidence exact matches (0.95+) are reliable and don't need Haiku validation\n        should_run_haiku = match.confidence < 0.95 or match.method in ['fuzzy', 'semantic']\n\n        if should_run_haiku:\n            print(f\"DEBUG: Triggering Haiku analysis (confidence={match.confidence:.2f}, method={match.method})\", file=sys.stderr)\n        else:\n            print(f\"DEBUG: Skipping Haiku analysis (high-confidence {match.method} match: {match.confidence:.2f})\", file=sys.stderr)\n\n        # Try to get Haiku analysis for better suggestions (only if needed)\n        if should_run_haiku and engineer.is_available():\n            print(f\"DEBUG: Running Haiku analysis...\", file=sys.stderr)\n            available_commands = load_available_commands()\n\n            # Track Haiku analysis latency\n            import time\n\n            haiku_start = time.perf_counter()\n\n            haiku_analysis = engineer.analyze_and_enhance(\n                prompt=prompt,\n                detected_command=match.command,\n                confidence=match.confidence,\n                available_commands=available_commands,\n                timeout=30,\n            )\n\n            haiku_latency_ms = (time.perf_counter() - haiku_start) * 1000\n\n            if haiku_analysis:\n                print(\n                    f\"DEBUG: Haiku analysis: {json.dumps(haiku_analysis)}\",\n                    file=sys.stderr,\n                )\n            else:\n                print(f\"DEBUG: Haiku analysis failed or timed out\", file=sys.stderr)\n        elif not should_run_haiku:\n            print(f\"DEBUG: Haiku analysis skipped (selective triggering)\", file=sys.stderr)\n        else:\n            print(\n                \"DEBUG: Claude Code CLI not available, skipping Haiku analysis\",\n                file=sys.stderr,\n            )\n\n        # AUGMENT MODE: Modify prompt with skill/command suggestion for reliability\n        print(\"DEBUG: Augmenting prompt for Claude\", file=sys.stderr)\n\n        # Create augmented prompt with skill suggestion\n        augmented_prompt = create_skill_augmented_prompt(match, prompt)\n\n        # Format interactive suggestion with Haiku analysis (if available)\n        interactive_msg = format_interactive_suggestion(\n            match, haiku_analysis, detection_count\n        )\n\n        # Determine the best command to use\n        # If Haiku suggests alternatives and it's not the best match, use the first alternative\n        best_command = match.command\n        correction_accepted = False\n\n        if haiku_analysis and not haiku_analysis.get(\"is_best_match\", True):\n            alternatives = haiku_analysis.get(\"alternatives\", [])\n            if alternatives:\n                best_command = alternatives[0]\n                correction_accepted = True\n                print(\n                    f\"DEBUG: Haiku suggests using {best_command} instead of {match.command}\",\n                    file=sys.stderr,\n                )\n\n        # Log correction to observability DB\n        if haiku_analysis:\n            try:\n                db = ObservabilityDB(\".promptune/observability.db\")\n\n                # Estimate token counts (rough approximation)\n                # Haiku prompt is ~150 tokens + command list + user prompt\n                prompt_tokens = (\n                    150 + len(prompt.split()) + len(load_available_commands()) * 5\n                )\n                # Response is typically ~50-100 tokens\n                completion_tokens = (\n                    50 + len(str(haiku_analysis.get(\"suggestion\", \"\"))) // 4\n                )\n\n                db.log_correction(\n                    original_command=match.command,\n                    corrected_command=best_command,\n                    original_confidence=match.confidence,\n                    correction_accepted=correction_accepted,\n                    model_name=\"haiku-4-5\",\n                    reasoning=haiku_analysis.get(\"suggestion\", \"\"),\n                    prompt_tokens=prompt_tokens,\n                    completion_tokens=completion_tokens,\n                    latency_ms=haiku_latency_ms,\n                    session_id=event.get(\"session_id\", \"\"),\n                    prompt_preview=prompt[:100],\n                )\n                print(\n                    f\"DEBUG: Logged correction to observability DB (accepted={correction_accepted})\",\n                    file=sys.stderr,\n                )\n            except Exception as e:\n                print(f\"DEBUG: Failed to log correction: {e}\", file=sys.stderr)\n\n        # Create augmented prompt with the best command (potentially corrected by Haiku)\n        if best_command != match.command:\n            # Use Haiku's suggested command\n            action = COMMAND_ACTIONS.get(best_command, \"complete this request\")\n            if best_command in SKILL_MAPPING:\n                skill_name = SKILL_MAPPING[best_command]\n                augmented_prompt = f\"{prompt}. You can use your {skill_name} skill to help with this task.\"\n            else:\n                augmented_prompt = (\n                    f\"{prompt}. Please use the {best_command} command to {action}.\"\n                )\n        else:\n            # Use original detection\n            augmented_prompt = create_skill_augmented_prompt(match, prompt)\n\n        # SUGGEST-ONLY MODE: Show detection but let user decide\n        # Don't auto-execute - just provide helpful suggestion\n        print(\n            f\"DEBUG: Using suggest-only mode - showing suggestion for {best_command}\",\n            file=sys.stderr,\n        )\n\n        response = {\n            \"continue\": True,\n            \"additionalContext\": interactive_msg,\n            \"suppressOutput\": False\n        }\n\n        print(f\"DEBUG: Response: {json.dumps(response)}\", file=sys.stderr)\n        print(json.dumps(response))\n\n    except Exception as e:\n        # Log error but don't block Claude\n        import traceback\n\n        print(f\"Promptune error: {e}\", file=sys.stderr)\n        print(f\"DEBUG: Traceback: {traceback.format_exc()}\", file=sys.stderr)\n        response = {\"continue\": True, \"suppressOutput\": True}\n        print(json.dumps(response))\n\n\nif __name__ == \"__main__\":\n    main()\n",
			"structuredPatch": [
				{
					"oldStart": 76,
					"oldLines": 7,
					"newStart": 76,
					"newLines": 7,
					"lines": [
						"         return self._model2vec",
						" ",
						"     def _get_semantic(self):",
						"-        if self._semantic is None:",
						"+        if self._semantic is None and SEMANTIC_ROUTER_AVAILABLE:",
						"             m = SemanticRouterMatcher()",
						"             self._semantic = m if m.is_available() else None",
						"         return self._semantic"
					]
				}
			],
			"userModified": false,
			"replaceAll": false
		},
		"tool_use_id": "toolu_01HiB9Lq42BxHkLVjrkW7w1c"
	},
	{
		"session_id": "9202596c-d296-440a-b61f-1fca4bc5b4ea",
		"transcript_path": "/Users/promptune/.claude/projects/-Users-promptune-DevProjects-promptune/9202596c-d296-440a-b61f-1fca4bc5b4ea.jsonl",
		"cwd": "/Users/promptune/DevProjects/promptune",
		"permission_mode": "acceptEdits",
		"hook_event_name": "PostToolUse",
		"tool_name": "TodoWrite",
		"tool_input": {
			"todos": [
				{
					"content": "Fix semantic_router hook crash with graceful error handling",
					"status": "completed",
					"activeForm": "Adding graceful error handling to hook"
				},
				{
					"content": "Show user how to use mo.hstack() for side-by-side layout",
					"status": "in_progress",
					"activeForm": "Demonstrating mo.hstack() usage"
				}
			]
		},
		"tool_response": {
			"oldTodos": [
				{
					"content": "Fix semantic_router hook crash with graceful error handling",
					"status": "in_progress",
					"activeForm": "Adding graceful error handling to hook"
				},
				{
					"content": "Show user how to use mo.hstack() for side-by-side layout",
					"status": "pending",
					"activeForm": "Demonstrating mo.hstack() usage"
				}
			],
			"newTodos": [
				{
					"content": "Fix semantic_router hook crash with graceful error handling",
					"status": "completed",
					"activeForm": "Adding graceful error handling to hook"
				},
				{
					"content": "Show user how to use mo.hstack() for side-by-side layout",
					"status": "in_progress",
					"activeForm": "Demonstrating mo.hstack() usage"
				}
			]
		},
		"tool_use_id": "toolu_01N3gKg7KGRHSKqaz6HWgS5Q"
	},
	{
		"session_id": "9202596c-d296-440a-b61f-1fca4bc5b4ea",
		"transcript_path": "/Users/promptune/.claude/projects/-Users-promptune-DevProjects-promptune/9202596c-d296-440a-b61f-1fca4bc5b4ea.jsonl",
		"cwd": "/Users/promptune/DevProjects/promptune",
		"permission_mode": "acceptEdits",
		"hook_event_name": "PostToolUse",
		"tool_name": "Bash",
		"tool_input": {
			"command": "echo '{\"prompt\":\"test research query\"}' | uv run /Users/promptune/DevProjects/promptune/hooks/user_prompt_submit.py 2>&1 | head -20",
			"description": "Test hook after error handling fix"
		},
		"tool_response": {
			"stdout": "DEBUG: Promptune hook triggered with prompt: 'test research query'\nDEBUG: Processing prompt (should_process=True)\n\u2705 Loaded 21 items from JSON\nDEBUG: Detection result: IntentMatch(command='ctx:researcher', confidence=1.0, method='fuzzy', latency_ms=0.9159159380942583, matched_keywords=['research'])\nDEBUG: Wrote detection to observability DB: ctx:researcher (100% fuzzy)\nDEBUG: Command detected (detection #341)\nDEBUG: Triggering Haiku analysis (confidence=1.00, method=fuzzy)\nDEBUG: Running Haiku analysis...\nDEBUG: Haiku analysis: {\"is_best_match\": true, \"alternatives\": [], \"suggestion\": \"\\u2705 /ctx:research is perfect for this. It will investigate your topic using 3 parallel agents (~2 min, grounded in current context).\"}\nDEBUG: Augmenting prompt for Claude\nDEBUG: Logged correction to observability DB (accepted=False)\nDEBUG: Using suggest-only mode - showing suggestion for ctx:researcher\nDEBUG: Response: {\"continue\": true, \"additionalContext\": \"\\ud83c\\udfaf Detected: `ctx:researcher` (100% via fuzzy)\\n\\u26a1 Detection speed: 0.92ms\\n\\n\\ud83d\\udcac Suggestion: \\u2705 /ctx:research is perfect for this. It will investigate your topic using 3 parallel agents (~2 min, grounded in current context).\\n\\n\\ud83d\\udca1 Tip: \\ud83c\\udf89 340 detections! Type `/ctx:stats` to see your time & cost savings\", \"suppressOutput\": false}\n{\"continue\": true, \"additionalContext\": \"\\ud83c\\udfaf Detected: `ctx:researcher` (100% via fuzzy)\\n\\u26a1 Detection speed: 0.92ms\\n\\n\\ud83d\\udcac Suggestion: \\u2705 /ctx:research is perfect for this. It will investigate your topic using 3 parallel agents (~2 min, grounded in current context).\\n\\n\\ud83d\\udca1 Tip: \\ud83c\\udf89 340 detections! Type `/ctx:stats` to see your time & cost savings\", \"suppressOutput\": false}",
			"stderr": "",
			"interrupted": false,
			"isImage": false
		},
		"tool_use_id": "toolu_01K2iCPsKtZTWdYWujGUh7rC"
	},
	{
		"session_id": "9202596c-d296-440a-b61f-1fca4bc5b4ea",
		"transcript_path": "/Users/promptune/.claude/projects/-Users-promptune-DevProjects-promptune/9202596c-d296-440a-b61f-1fca4bc5b4ea.jsonl",
		"cwd": "/Users/promptune/DevProjects/promptune",
		"permission_mode": "acceptEdits",
		"hook_event_name": "PostToolUse",
		"tool_name": "TodoWrite",
		"tool_input": {
			"todos": [
				{
					"content": "Fix semantic_router hook crash with graceful error handling",
					"status": "completed",
					"activeForm": "Adding graceful error handling to hook"
				},
				{
					"content": "Show user how to use mo.hstack() for side-by-side layout",
					"status": "completed",
					"activeForm": "Demonstrating mo.hstack() usage"
				}
			]
		},
		"tool_response": {
			"oldTodos": [
				{
					"content": "Fix semantic_router hook crash with graceful error handling",
					"status": "completed",
					"activeForm": "Adding graceful error handling to hook"
				},
				{
					"content": "Show user how to use mo.hstack() for side-by-side layout",
					"status": "in_progress",
					"activeForm": "Demonstrating mo.hstack() usage"
				}
			],
			"newTodos": [
				{
					"content": "Fix semantic_router hook crash with graceful error handling",
					"status": "completed",
					"activeForm": "Adding graceful error handling to hook"
				},
				{
					"content": "Show user how to use mo.hstack() for side-by-side layout",
					"status": "completed",
					"activeForm": "Demonstrating mo.hstack() usage"
				}
			]
		},
		"tool_use_id": "toolu_01KUL8Y2CidisHDdXX5Fav83"
	}
]
